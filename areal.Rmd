---
title: "areal.Rmd"
output: html_document
editor_options: 
  chunk_output_type: console
---

From https://urldefense.proofpoint.com/v2/url?u=https-3A__slu-2Dopengis.github.io_areal_articles_areal.html&d=DwIGAg&c=HPMtquzZjKY31rtkyGRFnQ&r=H5zkCeRAM0PaTMwM8Fis6g&m=cMg6K4s0E7Idd0IXoII8ehGT8CMyJCV-yD034iBrig8&s=eggYdUy099dfsiM-y1rRAOGvEy94w_J-lgKQ6bRi7wA&e=

```{r}
library(areal)
library(sf)
library(dplyr)
```

Areal interpolation is the process of making estimates from a source set of polygons to an overlapping but incongruent set of target polygons. This is need if, for example, a researcher wants to derive population estimates for tornado damage paths from the Census Bureau's census tracts. Damage paths do not align with census tract boundaries, so areal interpolation needs to be used to produce estimates at the tornado level.

### Functions

Two function prefixes are used in **areal** to allow users to take advantage of RStudio's auto complete functionality.

* `ar_` - data and functions that are used for multiple interpolation methods
* `aw_` - functions that are used specifically for areal weighted interpolation

### Data

The package contains four overlapping data sets that can be used for practice.

* `ar_stl_race` (2017 ACS demographic counts at the census tract level; n = 106)
* `ar_stl_asthma` (2017 asthma rates at the census tract level; n = 106)
* `ar_stl_wards` (the 2010 political subdivisions in St. Louis; n = 28).
* `ar_stl_wards` Clipped (the 2010 political subdivisions in St. Louis clipped to the Mississippi River shoreline; n = 28).

Functions in the **areal** package assume that all data are available as `sf` objects. The `st_read()` function is useful for importing data stored in a variety of formats into `sf` objects. Creating these objects is a pre-requisite to passing the validation steps before interpolations are completed.

It is also recommended that all data are stored using a projected coordinate system as opposed to a geographic coordinate system like NAD 1983 or WGS 1984. The key is that all of our data is projected using the same system. The `st_transform()` function can be used for data re-projection.

Finally, it is suggested that unnecessary columns are removed from the data frame prior to projection. This can be done using tools from the **dplyr** package, which makes it easy to `select()` the columns and `filter()` the observations to focus the interpolation. Columns are renamed using `rename()`.

Once data have been converted to sf objects, re-projected into the same planar coordinate systems, and cleaned, they are ready for interpolation.

The data preparation process is described in greater detail in a dedicated vignette.

### Areal Weighted Interpolation

Areal weighted interpolation makes a single, albeit significant, assumption about your data - that populations are evenly distributed within the source data. Imagine you have a census tract with 3,000 residents. Areal weighted interpolation will assume that these 3,000 residents are evenly spread out within that tract. 

This may not matter in some tracts, but in tracts with large parks or dense housing developments alongside commercial buildings, this assumption is not likely to hold.

The primary means for conducting interpolations, and possibly the only function you may need from areal, is the `aw_interpolate()` function.

```{r}
aw_interpolate(ar_stl_wards, 
               tid = WARD, 
               source = ar_stl_race, 
               sid = "GEOID", 
               weight = "sum", 
               output = "sf", 
               extensive = "TOTAL_E")
```

The function is capable of spatially extensive and intensive interpolations, and can use two different strategies for calculating areal weights for extensive interpolations. It can also produce output as either `tibble` or `sf` objects. These options are all documented in both the function documentation (use `?aw_interpolate`) as well as in a dedicated vignette.

### Full example

Areal weighted interpolation is a technique for estimating the values for overlapping but incongruent polygon features. This article describes the `areal` package's approach to areal weighted interpolation. After providing a quick introduction to the technique, the options for `aw_interpolate()` are discussed and an example of interpolating data using the manual workflow is provided.

## Introduction to Areal Weighted Interpolation

Areal weighted interpolation is the simplest approach to estimating population values for overlapping polygons. It makes a significant and important assumption - that individuals are spread out *evenly* within the source features. This assumption quickly breaks down in the real world - areas that have commercial developments mixed in with residential housing, for example, or neighborhoods with a large city park. We do not always have access to this type of contextual data, however, and so areal weighted interpolation remains a popular choice. 

Areal weighted interpolation is a multi-step process. The `areal` package contains a number of example data sets that can be used to illustrate this process.

```{r}
race <- ar_stl_race                 # census tract population estimates
asthma <- ar_stl_asthma             # census tract asthma rate estimates
wards <- ar_stl_wards               # political boundaries
wardsClipped <- ar_stl_wardsClipped # political boundaries clipped to river
```

The boundaries for the `race` and `asthma` the data are the same - census tracts. When mapped, this is what the census tract and ward features look like.
```{r}
plot(race$geometry)
plot(wards$geometry)
```

### Step 1: Intersection

The first step with areal weighted interpolation is to intersect the data. Imagine one shapefile (we'll call this the "target") acting as a cookie cutter - subdividing the features of the other (which we'll call the "source") based on areas of overlap such that only those overlapping areas remain (this is important - if these shapefiles do not cover identical areas, those areas only covered by one shapefile will be lost). The number of new features created is entirely dependent on the shapes of the features in the source and target data sets.

```{r}
# print number of features in source
nrow(race)
# print number of features in target
nrow(wards)
# create intersect for example purposes
nrow(st_intersection(race, wards))
```

By intersecting these two data sets, we get a new data set with *n* = 287 features. The resulting `sf` object looks like 

```{r}
intersect_sf <- st_intersection(race, wards)
head(intersect_sf)
```

One by-product of the intersection process is that each intersected feature takes on the attributes of both the source and target data. The population value of interest from each source feature (for example, total population per tract or `TOTAL_E`), therefore exists as an attribute for each intersected feature. The identification numbers from both the source (`GEOID`) and the target data (`WARD`) are also applied.

```{r}
  data.frame(
    GEOID = c(29510101100, 29510101100, 29510101200, 29510101200),
    TOTAL_E = c(2510, 2510, 3545, 3545),
    WARD = c(11, 12, 12, 13)
  )
```

### Step 2: Areal Weights

We then calculate an areal weight for each intersected feature. Let:

* ${W}_{i} = \textrm{areal weight for intersected feature i}$
* ${A}_{i} = \textrm{area of intersected feature i}$
* ${A}_{j} = \textrm{total area of source feature j}$

$$ {W}_{i} = \frac { {A}_{i} }{ {A}_{j} } $$

Since ${A}_{j}$ is calculated using the source identification number, the first two observations from table above with the first four rows of intersected data would have the same value for ${A}_{j}$, and the second two observations would also share the same ${A}_{j}$. The resulting values for ${W}_{i}$ would therefore be:

```{r}
  data.frame(
    GEOID = c(29510101100, 29510101100, 29510101200, 29510101200),
    TOTAL_E = c(2510, 2510, 3545, 3545),
    WARD = c(11, 12, 12, 13),
    Ai = c(355702.9, 901331.1, 875554.7, 208612.1),
    Aj = c(1257034.0, 1257034.0, 1084166.8, 1084166.8),
    Wi = c(0.28297, 0.71703, 0.807583, 0.192417)
  )
```

### Step 3: Estimate Population

Next, we need to estimate the share of the population value that occupies the intersected feature. Let:

* ${E}_{i} = \textrm{estimated value for intersected feature } i$
* ${W}_{i} = \textrm{areal weight for intersected feature } i$
* ${V}_{j} = \textrm{population value for source feature } j$

$$ {E}_{i} = {V}_{j} \cdot {W}_{i} $$
Using our sample data, we therefore multiply the value (`TOTAL_E`) by the weight (`Wi`) to produce our `EST` estimate column:
```{r}
  data.frame(
    GEOID = c(29510101100, 29510101100, 29510101200, 29510101200),
    TOTAL_E = c(2510, 2510, 3545, 3545),
    WARD = c(11, 12, 12, 13),
    Ai = c(355702.9, 901331.1, 875554.7, 208612.1),
    Aj = c(1257034.0, 1257034.0, 1084166.8, 1084166.8),
    Wi = c(0.28297, 0.71703, 0.807583, 0.192417),
    EST = c(710.2547, 1799.745, 2862.882, 682.1182)
  )
```

### Step 4: Summarize Data

Finally, we summarize the data based on the target identification number. Let:
* ${G}_{k} = \textrm{sum of all estimated values for target feature } k$
* ${E}_{ik} = \textrm{estimated values from intersected features in } i \textrm{ within target feature } k$
$$ {G}_{k} = \sum{{E}_{ik}} $$

With our hypothetical data, the resulting table would therefore look like:
```{r}
  data.frame(
    WARD = c(11, 12, 13),
    EST = c(710.2547, 4662.627, 682.1182)
  )
```
This process is repeated for each of the *n* = 287 observations in the intersected data - areal weights are calculated, and the product of the areal weight the source value is summed based on the target identification number.

### Extensive Interpolations

The example above is a spatially *extensive* interpolation because it involves count data. In `areal`, these estimates are obtained using the `aw_interpolate()` function.
```{r}
aw_interpolate(wards, 
               tid = WARD, 
               source = race, 
               sid = GEOID, 
               weight = "sum", 
               output = "tibble", 
               extensive = "TOTAL_E")
```

For spatially extensive interpolations, a list of variable names should be supplied for the argument `extensive`. This can be a single variable name, such as in the example above, or a vector of variable names.

```{r}
aw_interpolate(wards, 
               tid = WARD, 
               source = race, 
               sid = GEOID, 
               weight = "sum", 
               output = "tibble", 
               extensive = c("TOTAL_E", "WHITE_E", "BLACK_E"))
```

This ability is a key feature of functions in the **areal** package - iteration is built in by default, eliminating the need for repeated table joins after interpolations are calculated.

### Calculating Weights for Extensive Interpolations

The `aw_interpolate()` function also uses an argument `weight =`. There are two options, `"sum"` and `"total"`. Each makes a different assumption about the nature of the data and the relationship between the `source` and `target` features. For *perfectly* overlapping data, the distinction between these two options should not matter. In practice, however, there are often deviations in our data even between features that should be perfectly congruous.

The `"sum"` approach to calculating weights assumes that 100% of the source data should be divided among the target features. When ${A}_{j}$ is calculated (see previous section), it is done by taking the sum of the areas for all intersected features ($i$) within a given source feature ($j$). Let:

* ${A}_{j} = \textrm{total area of source feature j}$
* ${A}_{ij} = \textrm{areas for intersected features in } i \textrm{ within source feature } j$

$$ {A}_{j} = \sum{{A}_{ij}} $$

On the other hand, the `"total"` approach to calculating weights assumes that, if a source feature is only covered by 99.88% of the target features, only 99.88% of the source target's data should be allocated to target features in the interpolation. When ${A}_{j}$ is created, the actual area of source feature $j$ is used.

#### Weights Example 1: Non-Overlap Due to Data Quality

In the example above, `race` and `wards` are products of two different agencies. The `aw_stl_wards` data is a product of the City of the St. Louis and is quite close to fully overlapping with the U.S. Census Bureau's TIGER boundaries for the city. However, there are a number of very small deviations at the edges where the ward boundaries are *smaller* than the tracts (but only just so). These deviations result in small portions of census tracts not fitting into any ward. 

We can see this in the weights that are used by `aw_interpolate()`. The `aw_preview_weights()` function can be used to return a preview of these areal weights. 
```{r}
aw_preview_weights(wards, 
                   tid = WARD, 
                   source = race, 
                   sid = GEOID, 
                   type = "extensive")
```

The first tract listed above has a total estimated population of 2510. The practical impact of the weights is that only `r 2510*.9988335` individuals will be allocated to wards if the `"total"` approach to calculating areal weights is used. If `"sum"` is used, on the other hand, all 2510 individuals would be allocated to wards. In this scenario, the `"sum"` approach makes more sense because, while the race and ward data do not overlap in practice, they *should* overlap since no tracts extend out of the city's boundaries. We therefore want to ensure that all individuals within each tract are allocated out to wards.

With spatially extensive interpolations that utilize the `"sum"` approach, the sum of the interpolated column should equal the sum of the original source data's column that was interpolated. This can be verified with `aw_verify()`.
```{r}
result <- aw_interpolate(wards, 
                         tid = WARD, 
                         source = race, 
                         sid = GEOID, 
                         weight = "sum", 
                         output = "tibble", 
                         extensive = "TOTAL_E")
aw_verify(source = race, 
          sourceValue = TOTAL_E, 
          result = result, 
          resultValue = TOTAL_E)
```

This check does *not* work with the `"total"` approach to areal weights:

```{r verify-fail}
result <- aw_interpolate(wards, 
                         tid = WARD, 
                         source = race, 
                         sid = GEOID, 
                         weight = "total", 
                         output = "tibble", 
                         extensive = "TOTAL_E")
aw_verify(source = race, 
          sourceValue = TOTAL_E, 
          result = result, 
          resultValue = TOTAL_E)
```

#### Weights Example 2: Non-Overlap Due to Differing Boundaries

We can use the `aw_stl_wardsClipped` data to illustrate a more extreme disparity between source and target data. The `aw_stl_wardsClipped` data have been modified so that the ward boundaries do not extend past the Mississippi River shoreline, which runs along the entire eastern boundary of the city. When we overlay them on the city's census tracts, all of the census tracts on the eastern side of the city extend outwards. 

The difference in weights in this example is more extreme:

```{r}
aw_preview_weights(wardsClipped, 
                   tid = WARD, 
                   source = race, 
                   sid = GEOID, 
                   type = "extensive")
```

Only 72.31% of tract `29510101800`, for example, falls within a ward. In many American cities that lie within larger counties, tract boundaries do not stop at the municipal boundaries in a way that is similar to the difference between tracts and the clipped wards here. In this scenario, we do not want to allocate every individual into our city of interest and the `"total"` approach to weights is appropriate. Not using `"total"` would result in an over-count of individuals in our city.

If, on the other hand, we believe that all of the individuals *should* be allocated into wards, using `"total"` in this case would result in a severe under-count of individuals. 

### Intensive Interpolations

Spatially *intensive* operations are used when the data to be interpolated are a ratio. An example of these data can be found in `ar_stl_asthma`, which contains asthma rates for each census tract in the city. The interpolation process is very similar to the spatially extensive workflow, except with how the areal weight is calculated. Instead of using the source data's area for reference, the *target* data's area is used in the denominator. Let:

* ${W}_{i} = \textrm{areal weight for intersected feature i}$
* ${A}_{i} = \textrm{area of intersected feature i}$
* ${A}_{ik} = \textrm{areas for intersected features in } i \textrm{ within target feature } k$

$$ {W}_{i} = \frac { {A}_{i} }{ \sum{{A}_{ik}} } $$

Like spatially extensive interpolations that use the `"sum"` approach, the weights for intensive interpolations should always be equal to 1 as well. 

```{r}
aw_preview_weights(wards, 
                   tid = WARD, 
                   source = asthma, 
                   sid = GEOID, 
                   type = "intensive")
```

We can calculate the intensive interpolation by specifying a variable name for the `intensive` argument in `aw_interpolate()` and omitting the `extensive` argument:

```{r}
aw_interpolate(wards, 
               tid = WARD, 
               source = asthma, 
               sid = GEOID, 
               weight = "sum", 
               output = "tibble", 
               intensive = "ASTHMA")
```

This gives us an estimate of the asthma rates at the ward level.

### Mixed Interpolations

The **areal** package also provides support for "mixed" interpolations where both spatially extensive and intensive interpolations need to be calculated. We specify a variable name or a vector of variable names for *both* the `intensive` and `extensive` arguments.
```{r}
# remove sf geometry
st_geometry(race) <- NULL
# create combined data
race %>%
  select(GEOID, TOTAL_E, WHITE_E, BLACK_E) %>%
  left_join(asthma, ., by = "GEOID") -> combinedData
# interpolate
aw_interpolate(wards, 
               tid = WARD, 
               source = combinedData, 
               sid = GEOID, 
               weight = "sum", 
               output = "tibble", 
               intensive = "ASTHMA",
               extensive = c("TOTAL_E", "WHITE_E", "BLACK_E"))
```

## Output Options
All of the above examples have created a tibble for output, but **areal** also supports the creation of `sf` objects.
```{r}
aw_interpolate(wards, 
               tid = WARD, 
               source = asthma, 
               sid = GEOID, 
               weight = "sum", 
               output = "sf", 
               intensive = "ASTHMA")
```

## Other Features of `aw_interpolate()`

The `sf` option will include all variables that are in the original target data. The `aw_interpolate()` function is pipe-able, allowing **tidyverse** workflows. For example, if we want to remove the `OBJECTID` and `AREA` columns this can be accomplished as follows.
```{r}
wards %>%
  select(-OBJECTID, -AREA) %>%
  aw_interpolate(tid = WARD, 
                 source = asthma, 
                 sid = GEOID, 
                 weight = "sum", 
                 output = "tibble",
                 intensive = "ASTHMA")
```

The functions also support non-standard evaluation, meaning that inputs can be either unquoted as they are above or quoted as follows.
```{r}
wards %>%
  select(-OBJECTID, -AREA) %>%
  aw_interpolate(tid = "WARD", 
                 source = asthma, 
                 sid = "GEOID", 
                 weight = "sum", 
                 output = "tibble", 
                 intensive = "ASTHMA")
```

This functionality is not available for the `intensive` and `extensive` arguments at this time.

###  Manual Workflow

The **areal** functions use sub-functions that are called by `aw_interpolate()` so that the interpolation process is not a "black box" but can be recreated manually. This is a diagnostic toolkit, with the final interpolations estimated using the simpler `aw_interpolate()` function once any issues have been identified and fixed.

#### Prepare the data

First, we'll prepare the data but retaining only the columns we are interested in from the source data using the `select()` function from **dplyr**.

```{r}
race <- select(ar_stl_race, GEOID, TOTAL_E)
wards <- select(wards, -OBJECTID, -AREA)
```

We want to be careful to retain both a column with a value to be interpolated (total population in this case, `TOTAL_E`) and a column with a unique identification number (`GEOID` in this case).

#### Intersect the data

The interpolation process begins with calculating the intersection between the source and target data. We use the function `aw_intersect()` to accomplish this.
```{r}
intersect <- wards %>%
  aw_intersect(source = race, 
               areaVar = "area")
intersect
```

Note that `aw_intersect()` automatically calculates the area of the intersected feature.

#### Calculate total area

Next, we apply the total area of our source features to our data using `aw_total()`. This will implement the correct areal weighting approach based on the `type` and `weight` arguments. We'll use the `"sum"` approach to areal weights here.
```{r}
intersect <- intersect %>%
  aw_total(source = race, 
           id = GEOID, 
           areaVar = "area", 
           totalVar = "totalArea",
           type = "extensive", 
           weight = "sum")
intersect
```

Changing `type` to `"intensive"` would be necessary for spatially intensive interpolations. Likewise, changing `weight` to `"total"` is necessary if areas that lack overlap should not be allocated into the target features.

#### Calculate areal weight

With the total weight in hand, we are ready to calculate the areal weight itself using `aw_weight()`.
```{r}
intersect <- intersect %>%
  aw_weight(areaVar = "area", 
            totalVar = "totalArea", 
            areaWeight = "areaWeight")
intersect
```

#### Calculate estimated population

We then multiply the value (`TOTAL_E`) by the weight (`areaWeight`) to get a population estimate for each intersected feature using `aw_calculate()`.
```{r}
intersect <- intersect %>%
  aw_calculate(value = TOTAL_E, 
               areaWeight = "areaWeight")
intersect
```

There is an optional `newVar` argument that can be used to store the estimates in a new column rather than in the existing `value` column.

#### Aggregate estimated population by target ID

Finally, we aggregate the estimated values by target features using `aw_aggregate()`:
```{r}
result <- intersect %>%
  aw_aggregate(target = wards, 
               tid = WARD, 
               interVar = TOTAL_E)
result
```

### Estimate total residential population in each tornado path during 2017

Here we practice by estimating the total residential population in each tornado path during 2017.

```{r}
library(dplyr)
library(sf)
if (!file.exists("1950-2017-torn-aspath")) { 
  download.file(url = "https://urldefense.proofpoint.com/v2/url?u=http-3A__www.spc.noaa.gov_gis_svrgis_zipped_1950-2D2017-2Dtorn-2Daspath.zip&d=DwIGAg&c=HPMtquzZjKY31rtkyGRFnQ&r=H5zkCeRAM0PaTMwM8Fis6g&m=cMg6K4s0E7Idd0IXoII8ehGT8CMyJCV-yD034iBrig8&s=H_8fFUKYa1cG9_qlarzr3NBnwy_QJXqWvDUWDahllao&e=",
                destfile = "tornado.zip", mode = "wb")
  unzip("temporary.zip")
  }

sfdf <- st_read(dsn = "1950-2017-torn-aspath") %>%
  filter(yr >= 2017)

sfdfP <- st_read(dsn = "1950-2017-torn-initpoint") %>%
  filter(yr >= 2017)
class(sfdf)
```

Replace the empty geometries from the `..torn-aspath` file with the POINT geometries from the `..torn-initpoint` file.
```{r}
eg <- st_is_empty(sfdf)
sfdf$geometry[eg] <- sfdfP$geometry[eg]

```

To perform geocomputions we need to set coordinate reference system. Here it is geographic. We transform the geographic coordinate reference system to a specific Lambert conic conformal projection.
```{r}
sfdf <- st_transform(sfdf, 
                     crs = "+proj=lcc +lat_1=60 +lat_2=30 +lon_0=-90 +units=m")
```

Buffer the geometries.
```{r}
sfdf <- sfdf %>%
  mutate(Width = wid * .9144,
         Length = len * 1609.34)
sfdfB <- st_buffer(sfdf, 
                   dist = sfdf$Width/2,
                   endCapStyle = 'ROUND')
```

Get an API key from https://urldefense.proofpoint.com/v2/url?u=http-3A__api.census.gov_data_key-5Fsignup.html&d=DwIGAg&c=HPMtquzZjKY31rtkyGRFnQ&r=H5zkCeRAM0PaTMwM8Fis6g&m=cMg6K4s0E7Idd0IXoII8ehGT8CMyJCV-yD034iBrig8&s=VoTSVmbOT-Vptv0H6mYOIB6-Jl4fYNmA5WdXa7qyfOM&e=
```{r}
library(tidycensus)
# census_api_key("fce1b34522174228a5835e377d3ae0cd80588461", 
#               install = TRUE)
readRenviron("~/.Renviron")
```

Available variables.
```{r}
v15 <- load_variables(2016, "acs5", cache = TRUE)
View(v15)
```

Get data. Here county-level unweighted population estimates (totals) for the country (`B00001_001`). If we want tracts or smaller then we need to specify by state with `state = `.
```{r}
county_pop <- get_acs(geography = "county",
                      year = 2015,
                      variables = "B00001_001",
                      geometry = TRUE)
county_pop <- st_transform(county_pop, 
                           crs = st_crs(sfdfB))
```

The cookie cutters are the simple feature _polygons_ defining the boundaries we want values interpolated to. This is defined as the target object and it is the first argument in the `aw_interpolate()` function. We need to identify each cookie cutter with a unique target id (`tid`).

The dough is the underlying simple feature _variable_ that we want interpolated. It is defined in a source simple feature data frame (`source =`). Each feature must have a unique id (`sid`). The variable is specified in the `extensive =` argument.

Here the cookie cutters are the damage path POLYGONS and the county-level population estimate is the dough.
```{r}
out <- aw_interpolate(sfdfB, 
               tid = "om", 
               source = county_pop, 
               sid = "GEOID", 
               extensive = "estimate",
               weight = "total", 
               output = "sf")
```

Consider only Illinois. Then compare per-tornado population estimates using county and tract level demographic data.
```{r}
county_pop_IL <- get_acs(state = "IL",
                      geography = "county",
                      year = 2015,
                      variables = "B00001_001",
                      geometry = TRUE)
county_pop_IL <- st_transform(county_pop_IL, 
                              crs = st_crs(sfdfB))

tract_pop_IL <- get_acs(state = "IL",
                       geography = "tract",
                       year = 2015,
                       variables = "B00001_001",
                       geometry = TRUE)
tract_pop_IL <- st_transform(tract_pop_IL, 
                             crs = st_crs(sfdfB))
```

```{r}
out_county <- aw_interpolate(sfdfB, 
                             tid = "om", 
                             source = county_pop_IL, 
                             sid = "GEOID", 
                             weight = "total", 
                             output = "sf",
                             extensive = "estimate")
range(out_county$estimate, na.rm = TRUE)

out_tract <- aw_interpolate(sfdfB, 
                            tid = "om", 
                            source = tract_pop_IL, 
                            sid = "GEOID", 
                            weight = "total", 
                            output = "sf",
                            extensive = "estimate")
range(out_tract$estimate, na.rm = TRUE)
```

```{r}
cor.test(out_tract$estimate, out_county$estimate)
```


