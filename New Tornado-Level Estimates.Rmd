---
title: "Tornado-Level Estimates using Areal"
author: "Tyler Fricker"
date: "1/14/2019"
output: html_document
editor_options: 
  chunk_output_type: console
---

Set working directory and load packages.
```{r}
library(tidyverse)
library(lubridate)
library(sf)
library(tmap)
library(USAboundaries)
library(rgeos)
library(rgdal)
library(areal)
library(tidycensus)
```

### Data and Methods

Tornado data:

Download the tornado data from the Storm Prediction Center (SPC) http://www.spc.noaa.gov/gis/svrgis/ and load the shapefile into R.
```{r}
#download.file(url = "http://www.spc.noaa.gov/gis/svrgis/zipped/1950-2017-torn-aspath.zip",
#              destfile = "tornado2017.zip")
#download.file(url = "http://www.spc.noaa.gov/gis/svrgis/zipped/1950-2017-torn-initpoint.zip",
#              destfile = "tornado2017.zip")
#unzip("tornado2017.zip")
TornL.sf <- read_sf(dsn = "1950-2017-torn-aspath",
                   stringsAsFactors = FALSE)
TornP.sf <- read_sf(dsn = "1950-2017-torn-initpoint",
                   stringsAsFactors = FALSE)
```

The Paths data set has missing geometries, while the points data set does not.
```{r}
any(is.na(st_dimension(TornL.sf)))
any(is.na(st_dimension(TornP.sf)))
```

Merge the two data sets. Insert point geometries where there are missing linestring geometries.
```{r}
Torn.sf <- TornL.sf
eg <- which(st_is_empty(Torn.sf))
Torn.sf$geometry[eg] <- TornP.sf$geometry[eg]
```

Remove tornadoes occurring in Hawaii, Alaska, and Puerto Rico and those occurring before 1994. That year marks the beginning of comprehensive WSR-88D radar coverage. For missing EF ratings use the modification rules (if/else) defined here: https://www.spc.noaa.gov/wcm/OneTor_F-scale-modifications.pdf
```{r}
Torn.sf <- Torn.sf %>%
  filter(yr >= 1995,
         !st %in% c("AK", "PR", "HI")) %>%
  mutate(mag = ifelse(mag == -9 & len <= 5, 0, mag),
         mag = ifelse(mag == -9 & len > 5, 1, mag))
```

Add a data/time column also add columns for path length, width, and area in metric units. Leave the time zone as native CDT.
```{r}
Torn.sf <- Torn.sf %>%
  mutate(dy = format(as.Date(date, format="%Y-%m-%d"), "%d"),
         DateTime = as.POSIXct(paste(yr, mo, dy, time), format = "%Y%m%d%H:%M:%S"),
         Hour = hour(DateTime),
         Year = year(DateTime),
         Length = len * 1609.34,
         Length = ifelse(Length == 0, min(Length[Length > 0]), Length), #takes care of zero length
         Width = wid * .9144,
         Width = ifelse(Width == 0, min(Width[Width > 0]), Width), #takes care of zero width
         Width = ifelse(Year >= 1995, Width * pi/4, Width), #takes care of change: avg to max
         cas = inj + fat,
         AreaPath = Length * Width,
         Ma = factor(month.abb[mo], levels = month.abb[1:12])) %>%
         filter(cas > 0) %>%
  sf::st_sf()
max(Torn.sf$yr)
```

Add energy dissipation per tornado.
```{r}
perc <- c(1, 0, 0, 0, 0, 0, 
          .772, .228, 0, 0, 0, 0,
          .616, .268, .115, 0, 0, 0,
          .529, .271, .133, .067, 0, 0,
          .543, .238, .131, .056, .032, 0,
          .538, .223, .119, .07, .033, .017)
percM <- matrix(perc, ncol = 6, byrow = TRUE)
threshW <- c(29.06, 38.45, 49.62, 60.8, 74.21, 89.41)
midptW <- c(diff(threshW)/2 + threshW[-length(threshW)], threshW[length(threshW)] + 7.5)
ef <- Torn.sf$mag + 1
EW3 <- numeric()
for(i in 1:length(ef)) EW3[i] = midptW^3 %*% percM[ef[i], ]
Torn.sf <- Torn.sf %>%
  mutate(ED = EW3 * AreaPath)
```

Add tornado paths by buffering tornado tracks by associated widths.

To perform geocomputions we need to set coordinate reference system. Here it is geographic. We transform the geographic coordinate reference system to a specific Lambert conic conformal projection.
```{r}
Torn.sf <- st_transform(Torn.sf, 
                     crs = "+proj=lcc +lat_1=60 +lat_2=30 +lon_0=-90 +units=m")
```

Buffer the geometries.
```{r}
TornB.sf <- st_buffer(Torn.sf, 
                   dist = Torn.sf$Width/2,
                   endCapStyle = 'ROUND')
```

Calculate the error in area between the estimated paths (AreaPath variable) and the buffered tornado tracks (tornado paths).
```{r}
((sum(TornB.sf$AreaPath) - sum(as.vector(st_area(TornB.sf))))/sum(Torn.sf$AreaPath)) * 100
```

Percent error is between 1.8% (square end caps) and 2.3% (round end caps).

Play with Areal Package and TidyCensus package. Start with a small set of the data and work up.

Subset all 2010--2016 tornadoes.
```{r}
Torn2016.sf <- TornB.sf %>%
  mutate(ID = 1:nrow(TornB.sf)) %>%
  filter(Year >= 2010 & Year <= 2016)
```

Subset only 2010 tornadoes.
```{r}
Torn2010.sf <- TornB.sf %>%
  mutate(ID = 1:nrow(TornB.sf)) %>%
  filter(Year == 2010)
```

Because 2010 ACS data is not working with tidycensus. Try 2015 tornadoes instead.
```{r}
Torn2015.sf <- TornB.sf %>%
  mutate(ID = 1:nrow(TornB.sf)) %>%
  filter(Year == 2015)

Torn2013.sf <- TornB.sf %>%
  mutate(ID = 1:nrow(TornB.sf)) %>%
  filter(Year == 2013)
```

Socioeconomic/Demographic data:

Load API and variables from ACS. Here we use the 2000 Census summary file 3, and 2010 ACS 5-year estimates.
```{r}
census_api_key("2814de123d5d0461a3347a42849d25106688daa8", install = TRUE, overwrite = TRUE)
v90 = load_variables(1990, "sf3", cache = FALSE)
v00 = load_variables(2000, "sf3", cache = FALSE)
v10 <- load_variables(2010, "acs5", cache = FALSE)
v15 <- load_variables(2015, "acs5", cache = FALSE)
```

Get Census tracts for all contiguous U.S. states. Begin with 2017 tract-level data.
```{r}
us <- unique(fips_codes$state)[c(1, 3:8, 10:11, 13:51)]

counties <- fips_codes %>%
  filter(state %in% us)

options(tigris_use_cache = TRUE)

us2017.sf = reduce(map(us, function(x) {
  get_acs(state = x,
          geography = "tract", 
          variables = ("B01001_001E"),
          year = 2017,
          output = "wide",
          geometry = TRUE)
}),
rbind
)

us2017.sf <- st_transform(us2017.sf, crs = st_crs(Torn2010.sf))
```

Then collect 2010 ACS tract-level data. Current bug with 2010--2014 ACS 5-years estimates.
```{r}
us2010.sf <- reduce(map(us, function(x) {
  get_acs(state = x,
          county = county_state$COUNTYFP,
          geography = "tract", 
          variables = ("B01001_001E"),
          year = 2011,
          output = "wide",
          geometry = TRUE)
}),
rbind
)

us2010.sf <- st_transform(us2010.sf, crs = st_crs(Torn2010.sf))
```

Try a work-around. This takes a long time. Split the census tracts into multiple sfdf (otherwise Census API will time out).

Create dataframes of all Tracts. The variables for the Census (2000) and ACS (2010) include:
----------------------------------------------------------------------------------

Total Population (P001001/B01001_001E)
-----------------------------------------
Male Population (P008002/B01001_002E)
Male Population (Under 17) (P008003-P008020/B01001_003E-B01001_006E)
Male Population (18-44) (P008021-P008029/B01001_007E-B01001_0014E)
Male Population (45-64) (P008030-P008034/B01001_015E-B01001_019E)
Male Population (Over 65) (P008035-P008040/B01001_020E-B01001_025E)
-----------------------------------------
Female Population (P008041/B01001_026E)
Female Population (Under 17) (P008042-P008059/B01001_027E-B01001_030E)
Female Population (18-44) (P008060-P008068/B01001_031E-B01001_038E)
Female Population (45-64) (P008069-P008073/B01001_039E-B01001_043E)
Female Population (Over 65) (P008074-P008079/B01001_044E-B01001_049E)
-----------------------------------------
White alone (P006002/B02001_002E)
-----------------------------------------
Black or African-American alone (P006003/B02001_003E)
-----------------------------------------
Household Median Income (P053001/B19013_001E)
-----------------------------------------
Housing Units (/B25024_001)
Mobile Homes (H030010/B25024_010E)
-----------------------------------------

Start with the first 9 states.
```{r}
my_states <- unique(fips_codes$state)[c(1, 3:8, 10:11)]

county_state <- tigris::counties(
  state = my_states,
  cb = TRUE,
  resolution = "20m",
  year = "2010",
  class = "sf"
)

tract_state <- tigris::tracts(
  state = "AR",
  county = county_state$COUNTYFP,
  cb = TRUE,
  year = "2010",
  class = "sf"
)

us2010A.sf <- reduce(
    map2(
    .x = county_state$STATEFP,
    .y = county_state$COUNTYFP,
    ~ get_acs(
        state = .x,
        county = .y,
        geography = "tract", 
        variables = c("B01001_001", "B01001_002", "B01001_003", "B01001_004", "B01001_005", "B01001_006", "B01001_007", "B01001_008", "B01001_009", "B01001_010", "B01001_011", "B01001_012", "B01001_013", "B01001_014", "B01001_015", "B01001_016", "B01001_017", "B01001_018", "B01001_019", "B01001_020", "B01001_021", "B01001_022", "B01001_023", "B01001_024", "B01001_025", "B01001_026", "B01001_027", "B01001_028", "B01001_029", "B01001_030", "B01001_031", "B01001_032", "B01001_033", "B01001_034", "B01001_035", "B01001_036", "B01001_037", "B01001_038", "B01001_039", "B01001_040", "B01001_041", "B01001_042", "B01001_043", "B01001_044", "B01001_045", "B01001_046", "B01001_047", "B01001_048", "B01001_049", "B02001_002", "B02001_003", "B19013_001", "B25024_001", "B25024_010"),
        year = 2010,
        output = "wide",
        geometry = TRUE
        )
      ),
    rbind
   )
```

Then add in the next 20 states.
```{r}
my_states <- unique(fips_codes$state)[c(13:33)]

county_state <- tigris::counties(
  state = my_states,
  cb = TRUE,
  resolution = "20m",
  year = "2010",
  class = "sf"
)

us2010B.sf <- reduce(
    map2(
    .x = county_state$STATEFP,
    .y = county_state$COUNTYFP,
    ~ get_acs(
        state = .x,
        county = .y,
        geography = "tract", 
        variables = ("B01001_001E"),
        year = 2010,
        output = "wide",
        geometry = TRUE
        )
      ),
    rbind
   )
```

Then add the last 20 states
```{r}
my_states <- unique(fips_codes$state)[c(34:51)]

county_state <- tigris::counties(
  state = my_states,
  cb = TRUE,
  resolution = "20m",
  year = "2010",
  class = "sf"
)

us2010C.sf <- reduce(
    map2(
    .x = county_state$STATEFP,
    .y = county_state$COUNTYFP,
    ~ get_acs(
        state = .x,
        county = .y,
        geography = "tract", 
        variables = ("B01001_001E"),
        year = 2010,
        output = "wide",
        geometry = TRUE
        )
      ),
    rbind
   )
```

Combine the simple features dataframes.
```{r}
x <-  rbind(us2010.sf, us2010B.sf)
us2010.sf <-  rbind(x, us2010C.sf)

us2010.sf <- st_transform(us2010.sf, crs = st_crs(Torn2010.sf))
```


Then collect 2000 Census tract-level data.
```{r}
us2000.sf = reduce(map(us, function(x) {
  get_decennial(state = x,
          geography = "tract", 
          variables = ("P001001"),
          year = 2000,
          output = "wide",
          geometry = TRUE)
}),
rbind
)

us2000.sf <- st_transform(us2000.sf, crs = st_crs(Torn2010.sf))
```

Then collect 1990 Census tract-level data. There is a bug in the tidycensus package related to changes in tract and county differences. For now, use county level data.
```{r}
us1990.sf = reduce(map(us, function(x) {
  get_decennial(state = x,
          geography = "tract",
          variables = ("P0010001"),
          year = 1990,
          output = "wide",
          geometry = TRUE)
}),
rbind
)

us1990.sf <- st_transform(us1990.sf, crs = st_crs(Torn2010.sf))
```

Try areal weighted interpolation

The cookie cutters are the simple feature _polygons_ defining the boundaries we want values interpolated to. This is defined as the target object and it is the first argument in the `aw_interpolate()` function. We need to identify each cookie cutter with a unique target id (`tid`).

The dough is the underlying simple feature _variable_ that we want interpolated. It is defined in a source simple feature data frame (`source =`). Each feature must have a unique id (`sid`). The variable is specified in the `extensive =` argument.

Here the cookie cutters are the damage path POLYGONS and the county-level population estimate is the dough.
```{r}
out <- aw_interpolate(Torn2016.sf, 
               tid = "ID", 
               source = us2010.sf, 
               sid = "GEOID", 
               extensive = "B01001_001E",
               weight = "total", 
               output = "sf")
```

Compare this estimate to our previous estimates

Import `SocialCorrelates.shp` containing the casualty-producing tornadoes 1995-2016 and estimates of socioeconomic and demographic data at the tornado level. Add `Date` and `hr` columns to the data frame.
```{r}
#unzip("SocialCorrelates.zip")
TornSC.sf <- st_read(dsn = "SocialCorrelatesRound", 
                    layer = "SocialCorrelatesRound", 
                    stringsAsFactors = FALSE) %>%
  mutate(Date = as.Date(date),
         hr = hour(DateTim)) %>%
  filter(Year >= 2010)
```

Compare with previous estimates
```{r}
# Find the missing tornadoes
which(!as.character(out$DateTime) %in% as.character(TornSC.sf$DateTim))

# Compare areas
((sum(as.vector(st_area(TornSC.sf))) - sum(as.vector(st_area(out[-403,]))))/ sum(as.vector(st_area(TornSC.sf)))) * 100 

# Percent error
((sum(TornSC.sf$TotlPpl) - sum(out$B01001_001E[-c(403)]))/sum(TornSC.sf$TotlPpl)) * 100

#Pearson Correlation
cor.test(TornSC.sf$TotlPpl, out$B01001_001E[-c(403)])

# RMSE
Diffpop = sum(TornSC.sf$TotlPpl) - sum(out$B01001_001E[-c(403)])
RMSEpop = sqrt((abs(Diffpop)^2)/605)
```

Find percent error for different years. Begin by interpolating estimates across the 2015 ACS, 2000 Census, and 1990 Census.
```{r}
# 2015
out2015 <- aw_interpolate(Torn2016.sf, 
               tid = "ID", 
               source = us2010.sf, 
               sid = "GEOID", 
               extensive = "B01001_001E",
               weight = "total", 
               output = "sf")

# 2000
out2000 <- aw_interpolate(Torn2016.sf, 
               tid = "ID", 
               source = us2000.sf, 
               sid = "GEOID", 
               extensive = "P001001",
               weight = "total", 
               output = "sf")

# 1990
out1990 <- aw_interpolate(Torn2016.sf, 
               tid = "ID", 
               source = us1990.sf, 
               sid = "GEOID", 
               extensive = "P0010001",
               weight = "total", 
               output = "sf")
```

```{r}
# 2014 tornado paths
df <-  data.frame(x = c(2015, 2000, 1990), 
                y = c((abs(sum(out2015$B01001_001E) - sum(out2015$B01001_001E))/ sum(out2015$B01001_001E)),  (abs(sum(out2000$P001001) - sum(out2015$B01001_001E))/ sum(out2000$P001001)), (abs(sum(out1990$P0010001) - sum(out2015$B01001_001E))/ sum(out1990$P0010001))))

df$y <- df$y * 100

# 2013 tornado paths (rerun code)
df2 <-  data.frame(x = c(2015, 2000, 1990), 
                y = c((abs(sum(out2015$B01001_001E[-57]) - sum(out2015$B01001_001E[-57]))/ sum(out2015$B01001_001E[-57])),  (abs(sum(out2000$P001001[-57]) - sum(out2015$B01001_001E[-57]))/ sum(out2000$P001001[-57])), (abs(sum(out1990$P0010001[-57]) - sum(out2015$B01001_001E[-57]))/ sum(out1990$P0010001[-57]))))

df2$y <- df2$y * 100

# 2012 tornado paths (rerun code)

df3 <-  data.frame(x = c(2015, 2000, 1990), 
                y = c((abs(sum(out2015$B01001_001E) - sum(out2015$B01001_001E))/ sum(out2015$B01001_001E)),  (abs(sum(out2000$P001001) - sum(out2015$B01001_001E))/ sum(out2000$P001001)), (abs(sum(out1990$P0010001) - sum(out2015$B01001_001E))/ sum(out1990$P0010001))))

df3$y <- df3$y * 100

# 2016 tornado paths (rerun code)

df4 <-  data.frame(x = c(2015, 2000, 1990), 
                y = c((abs(sum(out2015$B01001_001E) - sum(out2015$B01001_001E))/ sum(out2015$B01001_001E)),  (abs(sum(out2000$P001001) - sum(out2015$B01001_001E))/ sum(out2000$P001001)), (abs(sum(out1990$P0010001) - sum(out2015$B01001_001E))/ sum(out1990$P0010001))))

df4$y <- df4$y * 100

# Plot
ggplot() +
  geom_point(data = df2, aes(x = x, y = y, colour = "red")) +
  geom_point(data = df3, aes(x = x, y = y, colour = "blue")) +
  geom_point(data = df4, aes(x = x, y = y, colour = "green")) + 
  geom_point(data = df, aes(x = x, y = y)) +
  ylab("Percent Error") + xlab("Year of Data") + 
  theme_minimal() +
  theme(legend.position="none")

                
sum(out2015$B01001_001E)
sum(out2000$P001001)
sum(out1990$P0010001)
```







### OLD CODE

Add tornado paths by buffering tornado tracks by associated widths. Need to account for tornadoes without a path (same latitude and longitude).
```{r}
begin.coord <- data.frame(lon=Torn.sf$slon, lat=Torn.sf$slat)
end.coord <- data.frame(lon=Torn.sf$elon, lat=Torn.sf$elat)

l_sf <- vector("list", nrow(Torn.sf))
for (i in seq_along(l_sf)){
  l_sf[[i]] <- st_linestring(as.matrix(rbind(begin.coord[i, ], end.coord[i,])))
}
# Create simple feature geometry list column
l_sfc <- st_sfc(l_sf, crs = "+proj=longlat +datum=WGS84")
# Put new bounding boxes into the geometry column in the sf database and transform the simple features to new projection
Torn.sf$geometry <- l_sfc
st_transform(Torn.sf, crs = "+proj=lcc +lat_1=33 +lat_2=45 +lat_0=39 +lon_0=-96 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0")
```

```{r}
TornSLine = as(Torn.sf, "Spatial")
TornSLine = spTransform(TornSLine, CRS("+proj=lcc +lat_1=33 +lat_2=45 +lat_0=39 +lon_0=-96 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0"))
TornSPoly = gBuffer(TornSLine, byid = TRUE, width = TornSLine$Width/2, capStyle = "SQUARE")
TornSPoly.sf <- st_as_sf(TornSPoly)
```

Calculate the error in area between the estimated paths (AreaPath variable) and the buffered tornado tracks (tornado paths).
```{r}
((sum(Torn.sf$AreaPath) - gArea(TornSPoly))/sum(Torn.sf$AreaPath)) * 100
```

The error is less than 1/10 a percent. The buffer method works.

Plot casualty-producing tornadoes.
```{r}
sts <- state.name[!state.name %in% c("Alaska", "Hawaii")]
stateBorders <- us_states(states = sts)

tm_shape(stateBorders, projection ="+init=epsg:2163") +
  tm_borders() +
  tm_fill(col = "grey94") +
  tm_shape(TornSPoly) +
  tm_polygons(col = "black") +
  tm_format('World', legend.position = c("left", "bottom"),
                   attr.position = c("left", "bottom"),
                  legend.frame = FALSE) +
  #tm_format_Europe(legend.position = c("left", "bottom"),
  #                 attr.position = c("left", "bottom"),
  #                 legend.frame = TRUE) +
  tm_scale_bar(position = c("right", "bottom")) +
  #tm_compass(position = c("right", "bottom")) +
  tm_layout(frame = FALSE, attr.outside=TRUE)
```

Plot casualty-producing tornadoes in Ohio over census tracts.
```{r}
sts <- state.name[!state.name %in% c("Alaska", "Hawaii")]
stateBorders <- us_states(states = sts)
OH = stateBorders[stateBorders$stusps == "OH",]

tm_shape(stateBorders[stateBorders$statefp == 39,], projection ="+init=epsg:2163") +
  tm_borders() +
  tm_fill(col = "grey94") +
  tm_shape(TornOH.sf[10,]) +
  tm_polygons(col = "blue") +
  tm_shape(OH2010.sfdf) + 
  tm_polygons(col = "grey98", alpha = 0.2) +
  tm_format('World', legend.position = c("left", "bottom"),
                   attr.position = c("left", "bottom"),
                  legend.frame = FALSE) +
  #tm_format_Europe(legend.position = c("left", "bottom"),
  #                 attr.position = c("left", "bottom"),
  #                 legend.frame = TRUE) +
  tm_scale_bar(position = c("right", "bottom")) +
  #tm_compass(position = c("right", "bottom")) +
  tm_layout(frame = FALSE, attr.outside=TRUE)
```

```{r}
get_state_demographic_data <- function(the_state, the_year) {
  
  options(tigris_use_cache = TRUE) #keep chached version of data
  
  # get all counties in given state
  counties <- tidycensus::fips_codes %>%
    dplyr::filter(state == the_state)
  
  # loop over counties and get tracts for each county
  purrr::map(counties$county_code, 
       ~ get_acs(
         geography = 'tract',
         table = c("B19301"),
         state = the_state,
         county = .x,
         year = the_year,
         survey = 'acs5',
         geometry = TRUE)
       ) %>% 
    purrr::reduce(rbind)  # bind rows of all counties
}

get_state_demographic_data(us, 2010)
```
