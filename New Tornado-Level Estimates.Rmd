---
title: "Tornado-Level Estimates using Areal"
author: "Tyler Fricker"
date: "1/14/2019"
output: html_document
editor_options: 
  chunk_output_type: console
---

Set working directory and load packages.
```{r}
library(tidyverse)
library(lubridate)
library(sf)
library(tmap)
library(USAboundaries)
library(rgeos)
library(rgdal)
library(areal)
library(tidycensus)
library(ipumsr)
```

### Data and Methods

Tornado data:

Download the tornado data from the Storm Prediction Center (SPC) http://www.spc.noaa.gov/gis/svrgis/ and load the shapefile into R.
```{r}
#download.file(url = "http://www.spc.noaa.gov/gis/svrgis/zipped/1950-2017-torn-aspath.zip",
#              destfile = "tornado2017.zip")
#download.file(url = "http://www.spc.noaa.gov/gis/svrgis/zipped/1950-2017-torn-initpoint.zip",
#              destfile = "tornado2017.zip")
#unzip("tornado2017.zip")
TornL.sf <- read_sf(dsn = "1950-2017-torn-aspath",
                   stringsAsFactors = FALSE)
TornP.sf <- read_sf(dsn = "1950-2017-torn-initpoint",
                   stringsAsFactors = FALSE)
```

The Paths data set has missing geometries, while the points data set does not.
```{r}
any(is.na(st_dimension(TornL.sf)))
any(is.na(st_dimension(TornP.sf)))
```

Merge the two data sets. Insert point geometries where there are missing linestring geometries.
```{r}
Torn.sf <- TornL.sf
eg <- which(st_is_empty(Torn.sf))
Torn.sf$geometry[eg] <- TornP.sf$geometry[eg]
```

Remove tornadoes occurring in Hawaii, Alaska, and Puerto Rico and those occurring before 1994. That year marks the beginning of comprehensive WSR-88D radar coverage. For missing EF ratings use the modification rules (if/else) defined here: https://www.spc.noaa.gov/wcm/OneTor_F-scale-modifications.pdf
```{r}
Torn.sf <- Torn.sf %>%
  filter(yr >= 1995,
         !st %in% c("AK", "PR", "HI")) %>%
  mutate(mag = ifelse(mag == -9 & len <= 5, 0, mag),
         mag = ifelse(mag == -9 & len > 5, 1, mag))
```

Add a data/time column also add columns for path length, width, and area in metric units. Leave the time zone as native CDT.
```{r}
Torn.sf <- Torn.sf %>%
  mutate(dy = format(as.Date(date, format="%Y-%m-%d"), "%d"),
         DateTime = as.POSIXct(paste(yr, mo, dy, time), format = "%Y%m%d%H:%M:%S"),
         Hour = hour(DateTime),
         Year = year(DateTime),
         Length = len * 1609.34,
         Length = ifelse(Length == 0, min(Length[Length > 0]), Length), #takes care of zero length
         Width = wid * .9144,
         Width = ifelse(Width == 0, min(Width[Width > 0]), Width), #takes care of zero width
         Width = ifelse(Year >= 1995, Width * pi/4, Width), #takes care of change: avg to max
         cas = inj + fat,
         AreaPath = Length * Width,
         Ma = factor(month.abb[mo], levels = month.abb[1:12])) %>%
         filter(cas > 0) %>%
  sf::st_sf()
max(Torn.sf$yr)
```

Add energy dissipation per tornado.
```{r}
perc <- c(1, 0, 0, 0, 0, 0, 
          .772, .228, 0, 0, 0, 0,
          .616, .268, .115, 0, 0, 0,
          .529, .271, .133, .067, 0, 0,
          .543, .238, .131, .056, .032, 0,
          .538, .223, .119, .07, .033, .017)
percM <- matrix(perc, ncol = 6, byrow = TRUE)
threshW <- c(29.06, 38.45, 49.62, 60.8, 74.21, 89.41)
midptW <- c(diff(threshW)/2 + threshW[-length(threshW)], threshW[length(threshW)] + 7.5)
ef <- Torn.sf$mag + 1
EW3 <- numeric()
for(i in 1:length(ef)) EW3[i] = midptW^3 %*% percM[ef[i], ]
Torn.sf <- Torn.sf %>%
  mutate(ED = EW3 * AreaPath)
```

Add tornado paths by buffering tornado tracks by associated widths.

To perform geocomputions we need to set coordinate reference system. Here it is geographic. We transform the geographic coordinate reference system to a specific Lambert conic conformal projection.
```{r}
Torn.sf <- st_transform(Torn.sf, 
                     crs = "+proj=lcc +lat_1=60 +lat_2=30 +lon_0=-90 +units=m")
```

Buffer the geometries.
```{r}
TornB.sf <- st_buffer(Torn.sf, 
                   dist = Torn.sf$Width/2,
                   endCapStyle = 'ROUND')
```

Calculate the error in area between the estimated paths (AreaPath variable) and the buffered tornado tracks (tornado paths).
```{r}
((sum(TornB.sf$AreaPath) - sum(as.vector(st_area(TornB.sf))))/sum(Torn.sf$AreaPath)) * 100
```

Percent error is between 1.8% (square end caps) and 2.3% (round end caps).

Subset tornadoes from 1995--2016
```{r}
TornB.sf <- TornB.sf %>%
  mutate(ID = 1:nrow(TornB.sf)) %>%
  filter(Year <= 2016)
```

Play with Areal Package and TidyCensus package. Start with a small set of the data and work up. 

**This code has been run and is available starting on line 665**

Socioeconomic/Demographic data:

Load API and variables from ACS. Here we use the 2000 Census summary file 3, and 2010 ACS 5-year estimates.
```{r}
census_api_key("2814de123d5d0461a3347a42849d25106688daa8", install = TRUE, overwrite = TRUE)
v90 = load_variables(1990, "sf3", cache = FALSE)
v00 = load_variables(2000, "sf3", cache = FALSE)
v10 <- load_variables(2010, "acs5", cache = FALSE)
v15 <- load_variables(2015, "acs5", cache = FALSE)
```

Create dataframes of all Tracts. The variables for the Census (2000) and ACS (2010) include:
----------------------------------------------------------------------------------

Total Population (P001001/B01001_001E)
-----------------------------------------
Male Population (P008002/B01001_002E)
Male Population (Under 17) (P008003-P008020/B01001_003E-B01001_006E)
Male Population (18-44) (P008021-P008029/B01001_007E-B01001_0014E)
Male Population (45-64) (P008030-P008034/B01001_015E-B01001_019E)
Male Population (Over 65) (P008035-P008040/B01001_020E-B01001_025E)
-----------------------------------------
Female Population (P008041/B01001_026E)
Female Population (Under 17) (P008042-P008059/B01001_027E-B01001_030E)
Female Population (18-44) (P008060-P008068/B01001_031E-B01001_038E)
Female Population (45-64) (P008069-P008073/B01001_039E-B01001_043E)
Female Population (Over 65) (P008074-P008079/B01001_044E-B01001_049E)
-----------------------------------------
White alone (P006002/B02001_002E)
-----------------------------------------
Black or African-American alone (P006003/B02001_003E)
-----------------------------------------
Household Median Income (P053001/B19013_001E)
-----------------------------------------
Housing Units (H030001/B25024_001)
Mobile Homes (H030010/B25024_010E)
-----------------------------------------

Try downloading shapefile for Census tracts from IPUMS GIS.
```{r}
Tracts_2010.sf <- read_sf(dsn = "2012_Tracts",
                   stringsAsFactors = FALSE) %>%
  mutate(STATEFP = as.numeric(STATEFP))
```

Import IPUMS data
```{r}
us2010 <- read.csv("2012ACS.csv", header = TRUE) %>%
  mutate(GISJOIN = as.character(GISJOIN))
```

Merge the two data sets and remove data from states outside of the contiguous U.S.
```{r}
US2010.sf <- ipums_shape_inner_join(us2010, Tracts_2010.sf, by = "GISJOIN") %>%
  mutate(ID2 = 1:nrow(.)) %>%
     st_as_sf()      

us <- unique(fips_codes$state_code)[c(1, 3:8, 10:11, 13:51)]
# Take care of single digit state code
us[1] = 1; us[2] = 4; us[3] = 5; us[4] = 6; us[5] = 8; us[6] = 9

US2010.sf <- US2010.sf[US2010.sf$STATEA %in% us,]

US2010.sf <- st_transform(US2010.sf, crs = st_crs(TornB.sf))
```

** This fixes the problem with tidycensus for 2010 ** 

Removing states outside of the contiguous US changes the number of tracts from 74001 to 72359.

Now focus on cleaning up the sf object (combine ages and margins of error)
```{r}
US2010.sf = US2010.sf %>%
  mutate(TotalPop_2010 = TotalPop,
         TotalMale_2010 = TotalMale,
         MaleUnder18_2010 = MaleUnder5 + Male5.9 + Male10.14 + Male15.17,
         Male18to44_2010 = Male18.19 + Male20 + Male21 + Male22.24 + Male25.29 + Male30.34
         + Male35.39 + Male40.44,
         Male45to64_2010 = Male45.49 + Male50.54 + Male55.59 + Male60.61 + Male62.64,
         MaleOver65_2010 = Male65.66 + Male67.69 + Male70.74 + Male75.79 + Male80.84
         + MaleOver85,
         TotalFemale_2010 = TotalFemale,
         FemaleUnder18_2010 = FemaleUnder5 + Female5.9 + Female10.14 + Female15.17,
         Female18to44_2010 = Female18.19 + Female20 + Female21 + Female22.24 + Female25.29
         + Female30.34 + Female35.39 + Female40.44,
         Female45to64_2010 = Female45.49 + Female50.54 + Female55.59 + Female60.61 
         + Female62.64,
         FemaleOver65_2010 = Female65.66 + Female67.69 + Female70.74 + Female75.79 
         + Female80.84 + FemaleOver85,
         White_2010 = White,
         Black_2010 = Black,
         TotalHousingUnits_2010 = TotalHousingUnits,
         MobileHomes_2010 = MobileHomes,
         MedianIncome_2010 = MedianIncome * 1.032,
         MedianIncomeM = MedianIncomeM * 1.032)
```

Try areal weighted interpolation

The cookie cutters are the simple feature _polygons_ defining the boundaries we want values interpolated to. This is defined as the target object and it is the first argument in the `aw_interpolate()` function. We need to identify each cookie cutter with a unique target id (`tid`).

The dough is the underlying simple feature _variable_ that we want interpolated. It is defined in a source simple feature data frame (`source =`). Each feature must have a unique id (`sid`). The variable is specified in the `extensive =` argument.

Here the cookie cutters are the damage path POLYGONS and the county-level population estimate is the dough.
```{r}
out2010.sf <- aw_interpolate(TornB.sf, 
               tid = "ID", 
               source = US2010.sf, 
               sid = "ID2",
               extensive = c("TotalPop_2010", "TotalMale_2010", "MaleUnder18_2010",
                             "Male18to44_2010", "Male45to64_2010", "MaleOver65_2010",
                             "TotalFemale_2010", "FemaleUnder18_2010",
                             "Female18to44_2010", "Female45to64_2010",
                             "FemaleOver65_2010", "White_2010",
                             "Black_2010", "TotalHousingUnits_2010", "MobileHomes_2010"),
               intensive = c("MedianIncome_2010"),
               weight = "total", 
               output = "sf")

```

Collect 2000 Census Tract information
```{r}
us = unique(fips_codes$state)[c(1, 3:8, 10:11, 13:51)]

us2000.sf = reduce(map(us, function(x) {
  get_decennial(state = x,
          geography = "tract", 
          variables = c("P001001", "P008002", "P008003", "P008004", "P008005", "P008006", "P008007", "P008008", "P008009", "P008010", "P008011", "P008012", "P008013", "P008014", "P008015", "P008016", "P008017", "P008018", "P008019", "P008020", "P008021", "P008022", "P008023", "P008024", "P008025", "P008026", "P008027", "P008028", "P008029", "P008030", "P008031", "P008032", "P008033", "P008034", "P008035", "P008036", "P008037", "P008038", "P008039", "P008040", "P008041", "P008042", "P008043", "P008044", "P008045", "P008046", "P008047", "P008048", "P008049", "P008050", "P008051", "P008052", "P008053", "P008054", "P008055", "P008056", "P008057", "P008058", "P008059", "P008060", "P008061", "P008062", "P008063", "P008064", "P008065", "P008066", "P008067", "P008068", "P008069", "P008070", "P008071", "P008072", "P008073", "P008074", "P008075", "P008076", "P008077", "P008078", "P008079", "P006002", "P006003",  "P053001", "H030001","H030010"),
          year = 2000,
          output = "wide",
          geometry = TRUE)
}),
rbind
)
```

Clean up sf object
```{r}
US2000.sf = us2000.sf %>%
  mutate(TotalPop_2000 = P001001,
         TotalMale_2000 = P008002,
         MaleUnder18_2000 = P008003 + P008004 + P008005 + P008006 + P008007 + P008008 
         + P008009 + P008010 + P008011 + P008012 + P008013 + P008014 + P008015 + P008016 
         + P008017 + P008018 + P008019 + P008020,
         Male18to44_2000 = P008021 + P008022 + P008023 + P008024 + P008025 
         + P008026 + P008027 + P008028 + P008029,
         Male45to64_2000 = P008030 + P008031 + P008032 + P008033 + P008034,
         MaleOver65_2000 = P008035 + P008036 + P008037 + P008038 + P008039 
         + P008040,
         TotalFemale_2000 = P008041,
         FemaleUnder18_2000 = P008042 + P008043 + P008044 + P008045 + P008046 + P008047 
         + P008048 + P008049 + P008050 + P008051 + P008052 + P008053 + P008054 + P008055 
         + P008056 + P008057 + P008058 + P008059,
         Female18to44_2000 = P008060 + P008061 + P008062 + P008063 + P008064 + P008065 
         + P008066 + P008067 + P008068,
         Female45to64_2000 = P008069+ P008070 + P008071 + P008072 + P008073,
         FemaleOver65_2000 = P008074 + P008075 + P008076 + P008077 + P008078 + P008079,
         White_2000 = P006002,
         Black_2000 = P006003,
         TotalHousingUnits_2000 = H030001,
         MobileHomes_2000 = H030010,
         MedianIncome_2000 = P053001 * 1.423)

US2000.sf = US2000.sf %>%
  mutate(ID2 = 1:nrow(us2000.sf))

US2000.sf <- st_transform(US2000.sf, crs = st_crs(TornB.sf))
```

```{r}
out2000.sf <- aw_interpolate(TornB.sf, 
               tid = "ID", 
               source = US2000.sf, 
               sid = "ID2",
               extensive = c("TotalPop_2000", "TotalMale_2000", "MaleUnder18_2000",
                             "Male18to44_2000", "Male45to64_2000", "MaleOver65_2000",
                             "TotalFemale_2000", "FemaleUnder18_2000",
                             "Female18to44_2000", "Female45to64_2000",
                             "FemaleOver65_2000", "White_2000","Black_2000",
                             "TotalHousingUnits_2000", "MobileHomes_2000"),
               intensive = c("MedianIncome_2000"),
               weight = "total", 
               output = "sf")
```

Collect 1990 Census Tract information. Use IPUMS again, because it is cleaner than tidycensus.
```{r}
Tracts_1990.sf <- read_sf(dsn = "1990_Tracts",
                   stringsAsFactors = FALSE)

us1990 <- read.csv("1990Census_1.csv", header = TRUE)

US1990.sf <- ipums_shape_inner_join(us1990, Tracts_1990.sf, by = "GISJOIN") %>%
  mutate(ID2 = 1:nrow(.)) %>%
     st_as_sf() 

us <- unique(fips_codes$state_code)[c(1, 3:8, 10:11, 13:51)]
# Take care of single digit state code
us[1] = 1; us[2] = 4; us[3] = 5; us[4] = 6; us[5] = 8; us[6] = 9

US1990.sf <- US1990.sf[US1990.sf$STATEA %in% us,]

US1990.sf <- st_transform(US1990.sf, crs = st_crs(TornB.sf))
```

Now focus on cleaning up the sf object
```{r}
US1990.sf = US1990.sf %>%
  mutate(TotalPop_1990 = TotalPop,
         TotalMale_1990 = TotalMale,
         MaleUnder18_1990 = round((PopUnder1 + Pop1.2 + Pop3.4 + Pop5 + Pop6 + Pop7.9 
         + Pop10.11 + Pop12.13 + Pop14 + Pop15 + Pop16 + Pop17) * .487),
         Male18to44_1990 = round((Pop18 + Pop19 + Pop20 + Pop21 + Pop22.24 + Pop25.29 
         + Pop30.34 + Pop35.39 + Pop40.44) * .487),
         Male45to64_1990 = round((Pop45.49 + Pop50.54 + Pop55.59 + Pop60.61 + Pop62.64) 
         * .487),
         MaleOver65_1990 = round((Pop65.69 + Pop70.74 + Pop75.79 + Pop80.84 + PopOver85) 
         * .487),
         TotalFemale_1990 = TotalFemale,
         FemaleUnder18_1990 = round((PopUnder1 + Pop1.2 + Pop3.4 + Pop5 + Pop6 + Pop7.9 
         + Pop10.11 + Pop12.13 + Pop14 + Pop15 + Pop16 + Pop17) * .513),
         Female18to44_1990 = round((Pop18 + Pop19 + Pop20 + Pop21 + Pop22.24 + Pop25.29 
         + Pop30.34 + Pop35.39 + Pop40.44) * .513),
         Female45to64_1990 = round((Pop45.49 + Pop50.54 + Pop55.59 + Pop60.61 + Pop62.64) 
         * .513),
         FemaleOver65_1990 = round((Pop65.69 + Pop70.74 + Pop75.79 + Pop80.84 + PopOver85)
         * .513),
         White_1990 = White,
         Black_1990 = Black,
         TotalHousingUnits_1990 = TotalHousingUnits,
         MobileHomes_1990 = MobileHomes,
         MedianIncome_1990 = round(MedianIncome * 1.9114))
```

```{r}
out1990.sf <- aw_interpolate(TornB.sf, 
               tid = "ID", 
               source = US1990.sf, 
               sid = "ID2",
               extensive = c("TotalPop_1990", "TotalMale_1990", "MaleUnder18_1990",
                             "Male18to44_1990", "Male45to64_1990", "MaleOver65_1990",
                             "TotalFemale_1990", "FemaleUnder18_1990",
                             "Female18to44_1990", "Female45to64_1990",
                             "FemaleOver65_1990", "White_1990","Black_1990",
                             "TotalHousingUnits_1990", "MobileHomes_1990"),
               intensive = c("MedianIncome_1990"),
               weight = "total", 
               output = "sf")
```

Now there are three sf objects with interpolated variables (out2010.sf, out2000.sf, out1990.sf). Linearily interpolate these values for the specific year of tornado occurrence.
```{r}
df = merge(data.frame(out1990.sf), data.frame(out2000.sf))
df = merge(df, data.frame(out2010.sf)) %>%
  arrange(ID) %>%
  mutate(geometry = out1990.sf$geometry)

# Find which columns to leave out
which(colnames(df) == "TotalPop_2010")
which(colnames(df) == "TotalPop_2000")
which(colnames(df) == "TotalPop_1990")

df1 = df[,1:65] %>%
  filter(Year <= 1999)

df2 = df[, c(1:33,50:81)] %>%
  filter(Year >= 2000)
```

Clean up the data frames. Start with missing 2000 data.
```{r}
# Total Pop
df1$TotalPop_2000[is.na(df1$TotalPop_2000)] = df1$TotalPop_1990[is.na(df1$TotalPop_2000)]

# Males
df1$TotalMale_2000[is.na(df1$TotalMale_2000)] = df1$TotalMale_1990[is.na(df1$TotalMale_2000)]
df1$MaleUnder18_2000[is.na(df1$MaleUnder18_2000)] = df1$MaleUnder18_1990[is.na(df1$MaleUnder18_2000)]
df1$Male18to44_2000[is.na(df1$Male18to44_2000)] = df1$Male18to44_1990[is.na(df1$Male18to44_2000)]
df1$Male45to64_2000[is.na(df1$Male45to64_2000)] = df1$Male45to64_1990[is.na(df1$Male45to64_2000)]
df1$MaleOver65_2000[is.na(df1$MaleOver65_2000)] = df1$MaleOver65_1990[is.na(df1$MaleOver65_2000)]

# Females
df1$TotalFemale_2000[is.na(df1$TotalFemale_2000)] = df1$TotalFemale_1990[is.na(df1$TotalFemale_2000)]
df1$FemaleUnder18_2000[is.na(df1$FemaleUnder18_2000)] = df1$FemaleUnder18_1990[is.na(df1$FemaleUnder18_2000)]
df1$Female18to44_2000[is.na(df1$Female18to44_2000)] = df1$Female18to44_1990[is.na(df1$Female18to44_2000)]
df1$Female45to64_2000[is.na(df1$Female45to64_2000)] = df1$Female45to64_1990[is.na(df1$Female45to64_2000)]
df1$FemaleOver65_2000[is.na(df1$FemaleOver65_2000)] = df1$FemaleOver65_1990[is.na(df1$FemaleOver65_2000)]

# White and Black
df1$White_2000[is.na(df1$White_2000)] = df1$White_1990[is.na(df1$White_2000)]
df1$Black_2000[is.na(df1$Black_2000)] = df1$Black_1990[is.na(df1$Black_2000)]

# Housing Units and Mobile Homes
df1$TotalHousingUnits_2000[is.na(df1$TotalHousingUnits_2000)] = df1$TotalHousingUnits_1990[is.na(df1$TotalHousingUnits_2000)]
df1$MobileHomes_2000[is.na(df1$MobileHomes_2000)] = df1$MobileHomes_1990[is.na(df1$MobileHomes_2000)]

# Median Income
df1$MedianIncome_2000[is.na(df1$MedianIncome_2000)] = df1$MedianIncome_1990[is.na(df1$MedianIncome_2000)]
```

Then move to missing 1990 data.
```{r}
# Total Pop
df1$TotalPop_1990[is.na(df1$TotalPop_1990)] = df1$TotalPop_2000[is.na(df1$TotalPop_1990)]

# Males
df1$TotalMale_1990[is.na(df1$TotalMale_1990)] = df1$TotalMale_2000[is.na(df1$TotalMale_1990)]
df1$MaleUnder18_1990[is.na(df1$MaleUnder18_1990)] = df1$MaleUnder18_2000[is.na(df1$MaleUnder18_1990)]
df1$Male18to44_1990[is.na(df1$Male18to44_1990)] = df1$Male18to44_2000[is.na(df1$Male18to44_1990)]
df1$Male45to64_1990[is.na(df1$Male45to64_1990)] = df1$Male45to64_2000[is.na(df1$Male45to64_1990)]
df1$MaleOver65_1990[is.na(df1$MaleOver65_1990)] = df1$MaleOver65_2000[is.na(df1$MaleOver65_1990)]

# Females
df1$TotalFemale_1990[is.na(df1$TotalFemale_1990)] = df1$TotalFemale_2000[is.na(df1$TotalFemale_1990)]
df1$FemaleUnder18_1990[is.na(df1$FemaleUnder18_1990)] = df1$FemaleUnder18_2000[is.na(df1$FemaleUnder18_1990)]
df1$Female18to44_1990[is.na(df1$Female18to44_1990)] = df1$Female18to44_2000[is.na(df1$Female18to44_1990)]
df1$Female45to64_1990[is.na(df1$Female45to64_1990)] = df1$Female45to64_2000[is.na(df1$Female45to64_1990)]
df1$FemaleOver65_1990[is.na(df1$FemaleOver65_1990)] = df1$FemaleOver65_2000[is.na(df1$FemaleOver65_1990)]

# White and Black
df1$White_1990[is.na(df1$White_1990)] = df1$White_2000[is.na(df1$White_1990)]
df1$Black_1990[is.na(df1$Black_1990)] = df1$Black_2000[is.na(df1$Black_1990)]

# Housing Units and Mobile Homes
df1$TotalHousingUnits_1990[is.na(df1$TotalHousingUnits_1990)] = df1$TotalHousingUnits_2000[is.na(df1$TotalHousingUnits_1990)]
df1$MobileHomes_1990[is.na(df1$MobileHomes_1990)] = df1$MobileHomes_2000[is.na(df1$MobileHomes_1990)]

# Median Income
df1$MedianIncome_1990[is.na(df1$MedianIncome_1990)] = df1$MedianIncome_2000[is.na(df1$MedianIncome_1990)]

df1 = na.omit(df1)
```

Read in the linear interpolation function
```{r}
lin_interpolate = function(x, y1, y2, data){
data = data
interp = numeric()
for(i in 1:nrow(data)){
  interp[i] = approx(x = x, 
                  y = c(y1[i], y2[i]), 
                  xout = data$Year[i],
                  rule = 2)$y
}
return(interp)
}
```

Run the function for 1990--1999 tornadoes
```{r}
# Total Population

interpPop = lin_interpolate(c(1990, 2000), df1$TotalPop_1990, df1$TotalPop_2000, df1)

# Male Population
interpMales = lin_interpolate(c(1990, 2000), df1$TotalMale_1990, df1$TotalMale_2000, df1)
interpMalesUnder18 = lin_interpolate(c(1990, 2000), df1$MaleUnder18_1990, df1$MaleUnder18_2000, df1)
interpMales18to44 = lin_interpolate(c(1990, 2000), df1$Male18to44_1990, df1$Male18to44_2000, df1)
interpMales45to64 = lin_interpolate(c(1990, 2000), df1$Male45to64_1990, df1$Male45to64_2000, df1)
interpMalesOver65 = lin_interpolate(c(1990, 2000), df1$MaleOver65_1990, df1$MaleOver65_2000, df1)

# Female Population
interpFemales = lin_interpolate(c(1990, 2000), df1$TotalFemale_1990, df1$TotalFemale_2000, df1)
interpFemalesUnder18 = lin_interpolate(c(1990, 2000), df1$FemaleUnder18_1990, df1$FemaleUnder18_2000, df1)
interpFemales18to44 = lin_interpolate(c(1990, 2000), df1$Female18to44_1990, df1$Female18to44_2000, df1)
interpFemales45to64 = lin_interpolate(c(1990, 2000), df1$Female45to64_1990, df1$Female45to64_2000, df1)
interpFemalesOver65 = lin_interpolate(c(1990, 2000), df1$FemaleOver65_1990, df1$FemaleOver65_2000, df1)

# White and Black
interpWhite = lin_interpolate(c(1990, 2000), df1$White_1990, df1$White_2000, df1)
interpBlack = lin_interpolate(c(1990, 2000), df1$Black_1990, df1$Black_2000, df1)

# Housing Units and Mobile Homes
interpHousingUnits = lin_interpolate(c(1990, 2000), df1$TotalHousingUnits_1990, df1$TotalHousingUnits_2000, df1)
interpMobileHomes = lin_interpolate(c(1990, 2000), df1$MobileHomes_1990, df1$MobileHomes_2000, df1)

# Median Income
interpInc = lin_interpolate(c(1990, 2000), df1$MedianIncome_1990, df1$MedianIncome_2000, df1)
```

Now pair the interpolations to variables in the dataframe.
```{r}
df1$TotalPop = interpPop
df1$TotalMale = interpMales
df1$MaleUnder18 = interpMalesUnder18
df1$Male18to44 = interpMales18to44
df1$Male45to64 = interpMales45to64
df1$MaleOver65 = interpMalesOver65
df1$TotalFemale = interpFemales
df1$FemaleUnder18 = interpFemalesUnder18
df1$Female18to44 = interpFemales18to44
df1$Female45to64 = interpFemales45to64
df1$FemaleOver65 = interpFemalesOver65
df1$White = interpWhite
df1$Black = interpBlack
df1$TotalHousingUnits = interpHousingUnits
df1$MobileHomes = interpMobileHomes
df1$MedianIncome = interpInc
```

Run the same code for the 2000--2010 data. Start with missing 2010 data.
```{r}
# Total Pop
df2$TotalPop_2010[is.na(df2$TotalPop_2010)] = df2$TotalPop_2000[is.na(df2$TotalPop_2010)]

# Males
df2$TotalMale_2010[is.na(df2$TotalMale_2010)] = df2$TotalMale_2000[is.na(df2$TotalMale_2010)]
df2$MaleUnder18_2010[is.na(df2$MaleUnder18_2010)] = df2$MaleUnder18_2000[is.na(df2$MaleUnder18_2010)]
df2$Male18to44_2010[is.na(df2$Male18to44_2010)] = df2$Male18to44_2000[is.na(df2$Male18to44_2010)]
df2$Male45to64_2010[is.na(df2$Male45to64_2010)] = df2$Male45to64_2000[is.na(df2$Male45to64_2010)]
df2$MaleOver65_2010[is.na(df2$MaleOver65_2010)] = df2$MaleOver65_2000[is.na(df2$MaleOver65_2010)]

# Females
df2$TotalFemale_2010[is.na(df2$TotalFemale_2010)] = df2$TotalFemale_2000[is.na(df2$TotalFemale_2010)]
df2$FemaleUnder18_2010[is.na(df2$FemaleUnder18_2010)] = df2$FemaleUnder18_2000[is.na(df2$FemaleUnder18_2010)]
df2$Female18to44_2010[is.na(df2$Female18to44_2010)] = df2$Female18to44_2000[is.na(df2$Female18to44_2010)]
df2$Female45to64_2010[is.na(df2$Female45to64_2010)] = df2$Female45to64_2000[is.na(df2$Female45to64_2010)]
df2$FemaleOver65_2010[is.na(df2$FemaleOver65_2010)] = df2$FemaleOver65_2000[is.na(df2$FemaleOver65_2010)]

# White and Black
df2$White_2010[is.na(df2$White_2010)] = df2$White_2000[is.na(df2$White_2010)]
df2$Black_2010[is.na(df2$Black_2010)] = df2$Black_2000[is.na(df2$Black_2010)]

# Housing Units and Mobile Homes
df2$TotalHousingUnits_2010[is.na(df2$TotalHousingUnits_2010)] = df2$TotalHousingUnits_2000[is.na(df2$TotalHousingUnits_2010)]
df2$MobileHomes_2010[is.na(df2$MobileHomes_2010)] = df2$MobileHomes_2000[is.na(df2$MobileHomes_2010)]

# Median Income
df2$MedianIncome_2010[is.na(df2$MedianIncome_2010)] = df2$MedianIncome_2000[is.na(df2$MedianIncome_2010)]
```

Next, move onto missing 2000 data.
```{r}
# Total Pop
df2$TotalPop_2000[is.na(df2$TotalPop_2000)] = df2$TotalPop_2010[is.na(df2$TotalPop_2000)]

# Males
df2$TotalMale_2000[is.na(df2$TotalMale_2000)] = df2$TotalMale_2010[is.na(df2$TotalMale_2000)]
df2$MaleUnder18_2000[is.na(df2$MaleUnder18_2000)] = df2$MaleUnder18_2010[is.na(df2$MaleUnder18_2000)]
df2$Male18to44_2000[is.na(df2$Male18to44_2000)] = df2$Male18to44_2010[is.na(df2$Male18to44_2000)]
df2$Male45to64_2000[is.na(df2$Male45to64_2000)] = df2$Male45to64_2010[is.na(df2$Male45to64_2000)]
df2$MaleOver65_2000[is.na(df2$MaleOver65_2000)] = df2$MaleOver65_2010[is.na(df2$MaleOver65_2000)]

# Females
df2$TotalFemale_2000[is.na(df2$TotalFemale_2000)] = df2$TotalFemale_2010[is.na(df2$TotalFemale_2000)]
df2$FemaleUnder18_2000[is.na(df2$FemaleUnder18_2000)] = df2$FemaleUnder18_2010[is.na(df2$FemaleUnder18_2000)]
df2$Female18to44_2000[is.na(df2$Female18to44_2000)] = df2$Female18to44_2010[is.na(df2$Female18to44_2000)]
df2$Female45to64_2000[is.na(df2$Female45to64_2000)] = df2$Female45to64_2010[is.na(df2$Female45to64_2000)]
df2$FemaleOver65_2000[is.na(df2$FemaleOver65_2000)] = df2$FemaleOver65_2010[is.na(df2$FemaleOver65_2000)]

# White and Black
df2$White_2000[is.na(df2$White_2000)] = df2$White_2010[is.na(df2$White_2000)]
df2$Black_2000[is.na(df2$Black_2000)] = df2$Black_2010[is.na(df2$Black_2000)]

# Housing Units and Mobile Homes
df2$TotalHousingUnits_2000[is.na(df2$TotalHousingUnits_2000)] = df2$TotalHousingUnits_2010[is.na(df2$TotalHousingUnits_2000)]
df2$MobileHomes_2000[is.na(df2$MobileHomes_2000)] = df2$MobileHomes_2010[is.na(df2$MobileHomes_2000)]

# Median Income
df2$MedianIncome_2000[is.na(df2$MedianIncome_2000)] = df2$MedianIncome_2010[is.na(df2$MedianIncome_2000)]

df2 = na.omit(df2)
```

Run the function for 2000--2016 tornadoes
```{r}
# Total Population

interpPop = lin_interpolate(c(2000, 2010), df2$TotalPop_2000, df2$TotalPop_2010, df2)

# Male Population
interpMales = lin_interpolate(c(2000, 2010), df2$TotalMale_2000, df2$TotalMale_2010, df2)
interpMalesUnder18 = lin_interpolate(c(2000, 2010), df2$MaleUnder18_2000, df2$MaleUnder18_2010, df2)
interpMales18to44 = lin_interpolate(c(2000, 2010), df2$Male18to44_2000, df2$Male18to44_2010, df2)
interpMales45to64 = lin_interpolate(c(2000, 2010), df2$Male45to64_2000, df2$Male45to64_2010, df2)
interpMalesOver65 = lin_interpolate(c(2000, 2010), df2$MaleOver65_2000, df2$MaleOver65_2010, df2)

# Female Population
interpFemales = lin_interpolate(c(2000, 2010), df2$TotalFemale_2000, df2$TotalFemale_2010, df2)
interpFemalesUnder18 = lin_interpolate(c(2000, 2010), df2$FemaleUnder18_2000, df2$FemaleUnder18_2010, df2)
interpFemales18to44 = lin_interpolate(c(2000, 2010), df2$Female18to44_2000, df2$Female18to44_2010, df2)
interpFemales45to64 = lin_interpolate(c(2000, 2010), df2$Female45to64_2000, df2$Female45to64_2010, df2)
interpFemalesOver65 = lin_interpolate(c(2000, 2010), df2$FemaleOver65_2000, df2$FemaleOver65_2010, df2)

# White and Black
interpWhite = lin_interpolate(c(2000, 2010), df2$White_2000, df2$White_2010, df2)
interpBlack = lin_interpolate(c(2000, 2010), df2$Black_2000, df2$Black_2010, df2)

# Housing Units and Mobile Homes
interpHousingUnits = lin_interpolate(c(2000, 2010), df2$TotalHousingUnits_2000, df2$TotalHousingUnits_2010, df2)
interpMobileHomes = lin_interpolate(c(2000, 2010), df2$MobileHomes_2000, df2$MobileHomes_2010, df2)

# Median Income
interpInc = lin_interpolate(c(2000, 2010), df2$MedianIncome_2000, df2$MedianIncome_2010, df2)
```

Now pair the interpolations to variables in the dataframe.
```{r}
df2$TotalPop = interpPop
df2$TotalMale = interpMales
df2$MaleUnder18 = interpMalesUnder18
df2$Male18to44 = interpMales18to44
df2$Male45to64 = interpMales45to64
df2$MaleOver65 = interpMalesOver65
df2$TotalFemale = interpFemales
df2$FemaleUnder18 = interpFemalesUnder18
df2$Female18to44 = interpFemales18to44
df2$Female45to64 = interpFemales45to64
df2$FemaleOver65 = interpFemalesOver65
df2$White = interpWhite
df2$Black = interpBlack
df2$TotalHousingUnits = interpHousingUnits
df2$MobileHomes = interpMobileHomes
df2$MedianIncome = interpInc
```

Merge the two data frames now
```{r}
# Find which columns to leave out
which(colnames(df1) == "TotalPop_1990")
which(colnames(df1) == "TotalPop")
which(colnames(df2) == "TotalPop_2000")


Early.df <- df1[,-c(34:65)]
Late.df <- df2[,-c(34:65)]

SocialCorrelates.sf <- rbind(Early.df, Late.df) %>%
  mutate(ID = 1:nrow(.)) %>%
  st_as_sf()

st_write(SocialCorrelates.sf, "SocialCorrelatesNew.shp")
```

Import `SocialCorrelatesNew.shp` containing the casualty-producing tornadoes 1995-2016 and estimates of socioeconomic and demographic data at the tornado level. Add `Date` and `hr` columns to the data frame.
```{r}
TornSC.sf <- st_read(dsn = "SocialCorrelatesNew", 
                    layer = "SocialCorrelatesNew", 
                    stringsAsFactors = FALSE)
```

Make new columns including total population by age
```{r}
TornSC.sf$popD = TornSC.sf$TotalPp/TornSC.sf$AreaPth * 10^6
TornSC.sf$PopUnder18 = TornSC.sf$MlUnd18 + TornSC.sf$FmlUn18
TornSC.sf$Pop18to44 = TornSC.sf$Ml18t44 + TornSC.sf$Fml1844
TornSC.sf$Pop45to64 = TornSC.sf$Ml45t64 + TornSC.sf$Fml4564
TornSC.sf$PopOver65 = TornSC.sf$MlOvr65 + TornSC.sf$FmlOv65
```

Maps of casualty producing tornadoes
```{r}
library(tmaptools)
#download.file("http://www2.census.gov/geo/tiger/GENZ2015/shp/cb_2015_us_state_20m.zip", destfile = "states.zip")
#unzip("states.zip")

us_geo <- read_shape("cb_2015_us_state_20m.shp", as.sf = TRUE, stringsAsFactors = FALSE)
us_geo = subset(us_geo, !(NAME %in% c("Alaska", "Hawaii", "Puerto Rico")))
colnames(us_geo)[5] = "st"

us_geo = us_geo %>%
  dplyr::select("st", "NAME" ,"geometry")

sfdf = TornSC.sf

tm_shape(us_geo, projection ="+init=epsg:2163") +
  tm_borders() +
  tm_shape(sfdf) +
  tm_polygons(col = "black") +
  tm_shape(us_geo) + 
  tm_borders(col = "grey", alpha = 0.3) +
  tm_format('World', legend.position = c("left", "bottom"),
                   attr.position = c("left", "bottom"),
                   legend.frame = FALSE) +
  #tm_compass(position = c("RIGHT", "bottom")) +
  tm_scale_bar(position = c("right", "bottom")) +
  tm_layout(frame = FALSE, attr.outside=TRUE)
```

Idealized model of the dasymetric procedure.
```{r}
EF0m = matrix(c(0, 0, 200, 200, 0, 0, 50, 50, 0, 0), 
              nrow = 5, ncol = 2)
EF1m = matrix(c(52.9, 52.9, 147.1, 147.1, 52.9, 13.225, 36.775, 36.775, 13.225, 13.225),
              nrow = 5, ncol = 2)
EF2m = matrix(c(80, 80, 120, 120, 80, 20, 30, 30, 20, 20), 
              nrow = 5, ncol = 2)
EF3m = matrix(c(93.3, 93.3, 106.7, 106.7, 93.3, 23.325, 26.675, 26.675, 23.325, 23.325), 
              nrow = 5, ncol = 2)
trackm = matrix(c(-20, 25, 220, 25), nrow = 2, ncol = 2, byrow = TRUE)

theta = -pi/6
rot = matrix(c(cos(theta), -sin(theta), sin(theta), cos(theta)),
             nrow = 2, ncol = 2, byrow = TRUE)
EF0r = EF0m %*% rot
EF1r = EF1m %*% rot
EF2r = EF2m %*% rot
EF3r = EF3m %*% rot
trackr = trackm %*% rot

EF0 = Polygons(list(Polygon(EF0r)), 0)
EF1 = Polygons(list(Polygon(EF1r)), 1)
EF2 = Polygons(list(Polygon(EF2r)), 2)
EF3 = Polygons(list(Polygon(EF3r)), 3)
spsNRC3 = SpatialPolygons(list(EF0, EF1, EF2, EF3))

spsNRC3A.df = fortify(spsNRC3)
spsNRC3A.df$EF = paste("EF", spsNRC3A.df$id, sep = "")

box1 = data.frame(long = c(-50, -50, 0, 0, -50),
                 lat = c(0, 50, 50, 0, 0))
box2 = data.frame(long = c(0, 0, 50, 50, 0),
                 lat = c(0, 50, 50, 0, 0))
box3 = data.frame(long = c(50, 50, 100, 100, 50),
                 lat = c(0, 50, 50, 0, 0))
box4 = data.frame(long = c(100, 100, 150, 150, 100),
                 lat = c(0, 50, 50, 0, 0))
box5 = data.frame(long = c(150, 150, 200, 200, 150),
                 lat = c(0, 50, 50, 0, 0))
box6 = data.frame(long = c(-50, -50, 0, 0, -50),
                 lat = c(50, 100, 100, 50, 50))
box7 = data.frame(long = c(0, 0, 50, 50, 0),
                 lat = c(50, 100, 100, 50, 50))
box8 = data.frame(long = c(50, 50, 100, 100, 50),
                 lat = c(50, 100, 100, 50, 50))
box9 = data.frame(long = c(100, 100, 150, 150, 100),
                 lat = c(50, 100, 100, 50, 50))
box10 = data.frame(long = c(150, 150, 200, 200, 150),
                 lat = c(50, 100, 100, 50, 50))
box11 = data.frame(long = c(-50, -50, 0, 0, -50),
                 lat = c(100, 150, 150, 100, 100))
box12 = data.frame(long = c(0, 0, 50, 50, 0),
                 lat = c(100, 150, 150, 100, 100))
box13 = data.frame(long = c(50, 50, 100, 100, 50),
                 lat = c(100, 150, 150, 100, 100))
box14 = data.frame(long = c(100, 100, 150, 150, 100),
                 lat = c(100, 150, 150, 100, 100))
box15 = data.frame(long = c(150, 150, 200, 200, 150),
                 lat = c(100, 150, 150, 100, 100))

box.df = rbind(box1, box2, box3, box4, box5, box6, box7, box8, box9, box10, box11, box12, box13, box14, box15)
box.df$inc = rep(c("< $10,000", "$10,000 - $20,000", "$30,001 - $40,000", "$10,000 - $20,000", "< $10,000", "< $10,000", "$20,001 - $30,000", "> $40,000", "$20,001 - $30,000", "< $10,000", "< $10,000", "$10,000 - $20,000", "$30,001 - $40,000", "$10,000 - $20,000", "< $10,000"), each = 5)
box.df$id = rep(c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15), each = 5)

lims = c("< $10,000", "$10,000 - $20,000", "$20,001 - $30,000", "$30,001 - $40,000", "> $40,000")
brks = c("< $10,000", "$10,000 - $20,000", "$20,001 - $30,000", "$30,001 - $40,000", "> $40,000") 
vals = c("#edf8e9", "#bae4b3", "#74c476", "#31a354", "#006d2c") 

ggplot(spsNRC3A.df[spsNRC3A.df$EF == "EF0", ], aes(x = long, y = lat)) +
  scale_x_continuous(limits = c(-50, 200)) +
  scale_y_continuous(limits = c(0, 150)) +
  coord_fixed() +
  geom_polygon(mapping = aes(long, lat, fill = factor(inc), group = id), data = box.df) +
  geom_polygon(mapping = aes(long, lat, fill = factor(inc), group = id), data = box.df, color = "white", show_guide=FALSE) +
  geom_polygon(fill = "transparent", color = "black") +
  scale_fill_manual(name = expression("Household Median Income\n(USD)"), limits = lims, breaks = brks, values = vals) +
  geom_segment(x = trackr[1, 1], xend = trackr[2, 1], y = trackr[1, 2], yend = trackr[2, 2],
               arrow = grid::arrow(length = grid::unit(.6, "cm"), type = "closed"), size = 1.0, color = "black") +
  xlab("") + ylab("") +
   theme(panel.background = element_blank(),
        panel.grid.major = element_line(colour = "gray", size = .25, linetype = 'dashed'),
        panel.grid.minor = element_blank(),
        axis.ticks = element_blank(),
        axis.text = element_blank(),
        strip.background = element_blank()) 
```

### Results

The procedure results in estimates of 12 variables that can be analyzed independently or in combination with the other attributes. 
```{r}
summary(TornSC.sf$TotalPp); summary(TornSC.sf$popD)

df = data.frame(TornSC.sf) %>%
  arrange(desc(TotalPp))
df = df[1:10,] %>%
  dplyr::select(date, st, inj, fat, cas, TotalPp)
df
```

For the set of 2,201 tornadoes, the median total population is 33.68 people with an interquartile range between 3.71 and 198 people. The median population density is 20.1 people per square kilometer with an interquartile range between 7.83 and 65.63 people per square kilometer. It is estimated that as many as 101,752 people were in the path of the Detroit, Michigan, EF2 tornado on 2 July 1997 resulting in 90 injuries. Of the top ten tornadoes ranked by total population, only one (1997 Detroit, Michigan tornado) is estimated to have impacted over 100,000 people. The next closest tornado (2013 St. Louis, Missouri tornado) is estimated to have impacted almost 70,000 fewer people at 36,840.

```{r}
summary(TornSC.sf$TotalMl); summary(TornSC.sf$TotlFml)
summary(TornSC.sf$White); summary(TornSC.sf$Black)
```

For the same set of tornadoes, the median number of males is 16.9 with an interquartile range between 1.9 and 98. Similarly, the median number of females is 16.9 with and interquartile range between 1.9 and 100. The median white population is 26.8 people with an interquartile range between 2.82 and 155 people, and the median black population is 0.85 people with an interquartile range between 0.04 and 13.2 people. On average, casualty-producing tornadoes have impacted nearly three times as many white people (344 people) as black people (130 people).

```{r}
summary(TornSC.sf$MdnIncm); summary(TornSC.sf$MoblHms)
df = data.frame(TornSC.sf) %>%
  arrange(desc(MoblHms))
df = df[1:10,] %>%
  dplyr::select(date, st, inj, fat, cas, MoblHms)
df
```

Additionally, the median household median income is \$46,988 with an interquartile range between \$39,559 and \$56,366 and the median number of mobile homes is 1.68 with an interquartile range between .18 and 9.5. It is estimated that as many as 821 mobile homes were in the path of the Hackleburg--Phil Campbell, Alabama EF5 tornado on 27 April 2011 resulting in 145 injuries and 72 deaths. Of the top 10 tornadoes ranked by the number of mobile homes, only two (1999 Bridge Creek--Moore, Oklahoma and 2012 Wichita, Kansas tornadoes) occurred in states outside of the Southeast. Five of the top 10 tornadoes ranked by the number of mobile homes occurred in the state of Alabama alone.

### Validation

Our method is validated by comparing results with demographic statistics of fatalities available in the Storm Events database (https://www.ncdc.noaa.gov/stormevents/). The database records all tornado segments from 1950 through 2016. The tornado segments are divided by county and each segment includes both an episode narrative and event narrative. Event fatality data are available within each event and include the type of death (direct or indirect) along with the age, sex, and location (if known) of the victim. No information about injuries is available.

To link the tornado in the SPC database with the associated tornado segments in the Storm Events database, we use information available in both data sources (e.g. state of occurrence, and number of deaths). For example, the 2011 Joplin, Missouri tornado caused 158 direct deaths. To find this tornado in the Storm Events database, we search for Missouri in the State/Area drop down menu. Next, we choose May 22, 2011 as the begin and end date, and tornado as the event type. After sorting by Death/Injury, the Storm Events database shows an event occurring in Jasper County, Missouri that caused 158 deaths. Choosing the hyper-linked location opens the Storm Events database, where information on the number of deaths, as well as the ages and sex of the fatalities exist.

```{r}
validation.df = data.frame(om = c(296616, 1147, 140, 634, 613600, 266, 133, 1253, 69, 850, 558, 893, 23, 22, 161, 289605, 833, 305268, 504758, 610234, 368708, 914, 877, 783),
                          date = c("2011-05-22", "1999-05-03", "2010-04-24", "2000-02-13", "2016-02-24", "2006-04-02", "2008-02-05", "2005-11-06", "1996-03-06", "1999-04-09", "2010-06-05", "1998-02-22", "2007-02-02", "2007-02-02", "2008-02-06", "2011-04-25", "1998-04-08", "2011-04-27", "2014-04-27", "2015-12-23", "2012-03-02", "2002-11-10", "1998-04-16", "1998-04-16") ,
                          fat = c(158, 36, 10, 11, 3, 16, 22, 24, 4, 4, 7, 25, 13, 8, 4, 4, 32, 20, 16, 9, 6, 4, 2, 2),
                          EstimatedMale = c(74.7, 17.7, 5.1, 6.1, 1.6, 7.8, 11, 11.6, 1.9, 2, 3.5, 12.7, 6, 3.8, 2.1, 2.1, 15.1, 9.9, 8.1, 4.4, 3, 2, 1, 1),
                          ActualMale = c(73, 15, 4, 2, 3, 9, 14, 11, 2, 3, 4, 13, 7, 3, 2, 3, 13, 10, 9, 4, 2, 3, 1, 2),
                          EstimatedFemale = c(83.3, 18.3, 4.9, 4.9, 1.4, 8.2, 11, 12.4, 2.1, 2, 3.5, 12.3, 7, 4.2, 1.9, 1.9, 16.9, 10.1, 7.9, 4.6, 3, 2, 1, 1),
                          ActualFemale = c(85, 21, 6, 9, 0, 7, 8, 13, 2, 1, 3, 12, 6, 5, 2, 1, 19, 10, 7, 5, 4, 1, 1, 0),
                          EstimatedUnder17 = c(33.9, 9.5, 2.6, 2.5, 0.4, 3.9, 5.4, 6.1, 1.1, 1.2, 1.6, 5.8, 3.3, 1, 0.8, 1, 7.4, 5.1, 4.2, 2.2, 1.5, 1.1, 0.5, 0.5),
                          ActualUnder17 = c(13, 3, 3, 2, 1, 2, 1, 4, 1, 0, 1, 1, 3, 1, 1, 0, 4, 3, 4, 1, 0, 0, 0, 0),
                          Estimated18to44 = c(48.9, 14.5, 3.5, 5.2, 1.2, 5.5, 7.5, 8.7, 1.5, 1.3, 2.2, 9.1, 4.1, 1.4, 1.1, 1.4, 11.4, 7.6, 5.6, 3.3, 1.9, 1.5, 0.7, 0.8),
                          Actual18to44 = c(32, 15, 1, 4, 1, 5, 7, 9, 0, 2, 3, 7, 3, 1, 0, 0, 5, 5, 3, 1, 1, 1, 0, 0),
                          Estimated45to64 = c(47.3, 8.4, 2.5, 2.2, 0.9, 4.4, 6.1, 6.1, 1, 1.1, 2.2, 6.5, 3.3, 2.4, 1.3, 1.1, 8.3, 5, 4.4, 2.3, 1.8, 0.9, 0.6, 0.5),
                          Actual45to64 = c(55, 10, 4, 5, 1, 4, 8, 9, 2, 2, 2, 7, 6, 0, 3, 4, 10, 8, 7, 3, 3, 0, 2, 1),
                          EstimatedOver65 = c(27.9, 3.6, 1.4, 1.1, 0.5, 2.2, 3, 3.1, 0.4, 0.4, 1, 3.6, 2.3, 3.2, 0.8, 0.5, 4.9, 2.3, 1.8, 1.2, 0.8, 0.5, 0.2, 0.2),
                          ActualOver65 = c(58, 8, 2, 0, 0, 5, 6, 2, 1, 0, 1, 10, 1, 6, 0, 0, 13, 4, 2, 4, 2, 3, 0, 1))

```

Estimated deaths by age and sex from our methodology are compared to observed deaths for two dozen tornadoes.

Begin with sex.
```{r}
A = validation.df %>%
  ggplot(aes(x = ActualMale, y = EstimatedMale)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1) +
  xlim(0, 90) +
  ylim(0, 90) +
  xlab("Observed Male Deaths") +
  ylab("Estimated Male Deaths") +
  theme_minimal()
#  theme_economist_white() + 
#  scale_colour_economist()

B = validation.df %>%
  ggplot(aes(x = ActualFemale, y = EstimatedFemale)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1) +
  xlim(0, 90) +
  ylim(0, 90) +
  xlab("Observed Female Deaths") +
  ylab("Estimated Female Deaths") +
  theme_minimal()
#  theme_economist_white() + 
#  scale_colour_economist()

```

Plot together.
```{r}
source("http://peterhaschke.com/Code/multiplot.R")
mat = matrix(c(1, 2), nrow = 1, byrow = TRUE)
A = A + ggtitle("A") + 
  theme(plot.title=element_text(hjust=0))
B = B + ggtitle("B") + 
  theme(plot.title=element_text(hjust=0))
multiplot(A, B, layout = mat)
```

Age groups.
```{r}
A = validation.df %>%
  ggplot(aes(x = ActualUnder17, y = EstimatedUnder17)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1) +
  xlim(0, 90) +
  ylim(0, 90) +
  xlab("Observed Deaths Age Under 18") +
  ylab("Estimated Deaths Age Under 18") +
  theme_minimal()
#  theme_economist_white() + 
#  scale_colour_economist()

B = validation.df %>%
  ggplot(aes(x = Actual18to44, y = Estimated18to44)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1) +
  xlim(0, 90) +
  ylim(0, 90) +
  xlab("Observed Deaths Age 18 to 44") +
  ylab("Estimated Deaths Age 18 to 44") +
  theme_minimal()
#  theme_economist_white() + 
#  scale_colour_economist()

C = validation.df %>%
  ggplot(aes(x = Actual45to64, y = Estimated45to64)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1) +
  xlim(0, 90) +
  ylim(0, 90) +
  xlab("Observed Deaths Age 45 to 64") +
  ylab("Estimated Deaths Age 45 to 64") +
  theme_minimal()
#  theme_economist_white() + 
#  scale_colour_economist()

D = validation.df %>%
  ggplot(aes(x = ActualOver65, y = EstimatedOver65)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1) +
  xlim(0, 90) +
  ylim(0, 90) +
  xlab("Observed Deaths Age Over 65") +
  ylab("Estimated Deaths Age Over 65") +
  theme_minimal()
#  theme_economist_white() + 
#  scale_colour_economist()
```

Plot together.
```{r}
source("http://peterhaschke.com/Code/multiplot.R")
mat = matrix(c(1, 2, 3, 4), nrow = 2, byrow = TRUE)
A = A + ggtitle("A") + 
  theme(plot.title=element_text(hjust=0))
B = B + ggtitle("B") + 
  theme(plot.title=element_text(hjust=0))
C = C + ggtitle("C") + 
  theme(plot.title=element_text(hjust=0))
D = D + ggtitle("D") + 
  theme(plot.title=element_text(hjust=0))
multiplot(A, B, C, D, layout = mat)
```

```{r}
cor.test(validation.df$EstimatedMale, validation.df$ActualMale)
cor.test(validation.df$EstimatedFemale, validation.df$ActualFemale)
cor.test(validation.df$EstimatedUnder17, validation.df$ActualUnder17)
cor.test(validation.df$Estimated18to44, validation.df$Actual18to44)
cor.test(validation.df$Estimated45to64, validation.df$Actual45to64)
cor.test(validation.df$EstimatedOver65, validation.df$ActualOver65)
```

The tornadoes were chosen to create a wide range of possible fatality estimates. Of these tornadoes, the average number of deaths is 18.3 with a minimum of 2 and a maximum of 158. The Pearson correlation between observed and estimated male deaths is 0.99 and the correlation between observed and estimated female deaths is 0.99, both indicating a very strong relationship. The Pearson correlation between observed and estimated deaths for people under the age of 17 is 0.93 and the correlation between observed and estimated deaths for people 18 to 44, people 45 to 64, and people over 65 is 0.97, 0.99, and 0.99, respectively.


### Discussion

Having accurate estimates of socioeconomic and demographic variables at the tornado level allows us to infer aggregate demographics. For instance, using the ratio of the white population relative to total population, the number of white casualties per tornado can be estimated. Similarly, using the ratio of the black population relative to total population, the number of black casualties per tornado can be inferred.
```{r}
sfdf = TornSC.sf %>%
  mutate(ratioW = White/TotalPp,
         WhiteCas = cas * ratioW,
         ratioB = Black/TotalPp,
         BlackCas = cas * ratioB) %>%
  arrange(desc(BlackCas))

sfdf2 = sfdf[1:10,]

sfdf = sfdf %>%
  arrange(desc(WhiteCas))

sfdf3 = sfdf[1:10,]

A = tm_shape(us_geo, projection ="+init=epsg:2163") +
  tm_borders() +
  tm_shape(sfdf3) +
  tm_bubbles("WhiteCas", col = "red", alpha = 0.3, sizes.legend = c(200, 400, 800, 1200), title.size="Estimated White Casualties", scale = 5/3) +
  tm_shape(us_geo) + 
  tm_borders(col = "grey", alpha = 0.3) +
  tm_style('white', title="A") +
  tm_format('World', legend.position = c("left", "bottom"),
                   attr.position = c("left", "bottom"),
                   legend.frame = FALSE) +
  #tm_compass(position = c("RIGHT", "bottom")) +
  tm_scale_bar(position = c("right", "bottom")) +
  tm_layout(frame = FALSE, attr.outside=TRUE)

B = tm_shape(us_geo, projection ="+init=epsg:2163") +
  tm_borders() +
  tm_shape(sfdf2) +
  tm_bubbles("BlackCas", col = "blue", sizes.legend = c(200, 400, 800, 1200), alpha = 0.3, title.size="Estimated Black Casualties", scale = 4/3) +
  tm_shape(us_geo) + 
  tm_borders(col = "grey", alpha = 0.3) +
  tm_style('white', title="B") +
  tm_format('World', legend.position = c("left", "bottom"),
                    attr.position = c("left", "bottom"),
                   legend.frame = FALSE) +
  tm_scale_bar(position = c("right", "bottom")) +
  tm_layout(frame = FALSE, attr.outside=TRUE)

tmap_arrange(A, B, ncol = 1)
```

The median number of white casualties for the set of 2,201 tornadoes is 1.96 people with an interquartile range of 0.97 and 5.61 people. In comparison, the median number of black casualties for the same set of tornadoes is 0.12 people with an interquartile range of 0.01 and 0.63. The average number of white casualties is 9.19 people and the average number of black casualties is 1.91 people. Of the top 10 tornadoes ranked by white casualties, three occurred in the state of Oklahoma, two in the state of Alabama, and one in Arkansas, Kentucky, Missouri, Georgia, and Texas. Of the top 10 tornadoes ranked by black casualties, only one (1997 Detroit, Michigan tornado) occurred in a state outside of the Southeast.

Using the ratio of the young population (under 17) relative to total population, the number of young casualties per tornado can be estimated. Similarly, using the ratio of the elderly population (over 65) relative to total population, the number of elderly casualties per tornado can be estimated.
```{r}
sfdf = TornSC.sf %>%
  mutate(ratioU = PopUnder18/TotalPp,
         YoungCas = cas * ratioU,
         ratioO = PopOver65/TotalPp,
         OldCas = cas * ratioO) %>%
  arrange(desc(YoungCas))

sfdf2 = sfdf[1:10,]

sfdf = sfdf %>%
  arrange(desc(OldCas))

sfdf3 = sfdf[1:10,]

A = tm_shape(us_geo, projection ="+init=epsg:2163") +
  tm_borders() +
  tm_shape(sfdf2) +
  tm_bubbles("YoungCas", col = "red", alpha = 0.3, sizes.legend = c(100, 200, 300, 400), title.size="Estimated Young Casualties", scale = 5/3) +
  tm_shape(us_geo) + 
  tm_borders(col = "grey", alpha = 0.3) +
  tm_style('white', title="A") +
  tm_format('World', legend.position = c("left", "bottom"),
                   attr.position = c("left", "bottom"),
                   legend.frame = FALSE) +
  #tm_compass(position = c("RIGHT", "bottom")) +
  tm_scale_bar(position = c("right", "bottom")) +
  tm_layout(frame = FALSE, attr.outside=TRUE)

B = tm_shape(us_geo, projection ="+init=epsg:2163") +
  tm_borders() +
  tm_shape(sfdf3) +
  tm_bubbles("OldCas", col = "blue", sizes.legend = c(100, 200, 300, 400), alpha = 0.3, title.size="Estimated Elderly Casualties", scale = 4/3) +
  tm_shape(us_geo) + 
  tm_borders(col = "grey", alpha = 0.3) +
  tm_style('white', title="B") +
  tm_format('World', legend.position = c("left", "bottom"),
                    attr.position = c("left", "bottom"),
                   legend.frame = FALSE) +
  tm_scale_bar(position = c("right", "bottom")) +
  tm_layout(frame = FALSE, attr.outside=TRUE)

tmap_arrange(A, B, ncol = 1)
```

The median number of young casualties for the set of 2,201 tornadoes is 0.64 people with an interquartile range of 0.28 and 1.78. In comparison, the median number of elderly casualties is 0.37 with an interquartile range of 0.17 and 0.99. The average number of young casualties is 2.91 and the average number of elderly casualties is 1.64. Of the top 10 tornadoes ranked by young and elderly casualties, only one (1998 Spencer, South Dakota tornado) occurred in a state outside the southern Great Plains or Southeast.

Having tornado-level aggregated socioeconomic and demographic information, the next step toward understanding tornado casualties is to examine how these factors are related to the number of deaths and injuries. For example, the Pearson correlation between estimated socioeconomic and demographic variables and the number of deaths and injuries are seen below.
```{r}
cor(TornSC.sf$fat, TornSC.sf$TotalPp); cor(TornSC.sf$inj, TornSC.sf$TotalPp)
cor(TornSC.sf$fat, TornSC.sf$popD); cor(TornSC.sf$inj, TornSC.sf$popD) 
cor(TornSC.sf$fat, TornSC.sf$TotalMl); cor(TornSC.sf$inj, TornSC.sf$TotalMl)
cor(TornSC.sf$fat, TornSC.sf$TotlFml); cor(TornSC.sf$inj, TornSC.sf$TotlFml)
cor(TornSC.sf$fat, TornSC.sf$White); cor(TornSC.sf$inj, TornSC.sf$White)
cor(TornSC.sf$fat, TornSC.sf$Black); cor(TornSC.sf$inj, TornSC.sf$Black)
cor(TornSC.sf$fat, TornSC.sf$MdnIncm); cor(TornSC.sf$inj, TornSC.sf$MdnIncm)
cor(TornSC.sf$fat, TornSC.sf$MoblHms); cor(TornSC.sf$inj, TornSC.sf$MoblHms)
cor(TornSC.sf$fat, TornSC.sf$PopUnder18); cor(TornSC.sf$inj, TornSC.sf$PopUnder18)
cor(TornSC.sf$fat, TornSC.sf$Pop18to44); cor(TornSC.sf$inj, TornSC.sf$Pop18to44)
cor(TornSC.sf$fat, TornSC.sf$Pop45to64); cor(TornSC.sf$inj, TornSC.sf$Pop45to64)
cor(TornSC.sf$fat, TornSC.sf$PopOver65); cor(TornSC.sf$inj, TornSC.sf$PopOver65)
```

The coefficients range between 0.01 and 0.48 for the number of deaths and between $-$0.01 and 0.42 for the number of injuries. A moderate correlation exists between the number of deaths and injuries for total population, number of males, number of females, white population, black population, and the number of mobile homes. While bi-variate correlations limit the ability to meaningfully interpret the effect a specific variable has on the number of deaths and injuries given the influence of omitted intervening variables, they do provide signals for which variables may play a role in statistically explaining the number of deaths and injuries. For example, the relatively high correlation with mobile homes is consistent with previous research, as is the relatively high correlation with race.

How climate change will influence tornado activity, and in turn tornado casualties remains an open and challenging question. Statistically, given a tornado that produces at least one casualty, the casualty rate depends on the number of people in harm's way and on the power of the winds inside the vortex. Using a regression model, Fricker et al. (2017) find that casualties increase by 33\% ($\pm$ 3\%) with a doubling of the tornado energy and that casualties increase by 21\% ($\pm$ 3\%) with a doubling of the number of people affected, on average. Including an interaction term in the regression model provides an even better description of casualties given population and energy. But these findings are only the beginning of the story as socioeconomic and demographic variables likely impact the casualty rate. Having estimates of these variables at the individual tornado level provides an opportunity to build a model to estimate casualty rates from changes in socioeconomic and demographic variables controlling for population and energy.


### OLD CODE
Compare this estimate to our previous estimates

Import `SocialCorrelates.shp` containing the casualty-producing tornadoes 1995-2016 and estimates of socioeconomic and demographic data at the tornado level. Add `Date` and `hr` columns to the data frame.
```{r}
#unzip("SocialCorrelates.zip")
TornSC.sf <- st_read(dsn = "SocialCorrelatesRound", 
                    layer = "SocialCorrelatesRound", 
                    stringsAsFactors = FALSE) %>%
  mutate(Date = as.Date(date),
         hr = hour(DateTim)) %>%
  filter(Year >= 2010)
```

Compare with previous estimates
```{r}
# Find the missing tornadoes
which(!as.character(out$DateTime) %in% as.character(TornSC.sf$DateTim))

# Compare areas
((sum(as.vector(st_area(TornSC.sf))) - sum(as.vector(st_area(out[-403,]))))/ sum(as.vector(st_area(TornSC.sf)))) * 100 

# Percent error
((sum(TornSC.sf$TotlPpl) - sum(out$B01001_001E[-c(403)]))/sum(TornSC.sf$TotlPpl)) * 100

#Pearson Correlation
cor.test(TornSC.sf$TotlPpl, out$B01001_001E[-c(403)])

# RMSE
Diffpop = sum(TornSC.sf$TotlPpl) - sum(out$B01001_001E[-c(403)])
RMSEpop = sqrt((abs(Diffpop)^2)/605)
```

Subset all 2010--2016 tornadoes.
```{r}
Torn2016.sf <- TornB.sf %>%
  mutate(ID = 1:nrow(TornB.sf)) %>%
  filter(Year >= 2010 & Year <= 2016)
```

Subset only 2010 tornadoes.
```{r}
Torn2010.sf <- TornB.sf %>%
  mutate(ID = 1:nrow(TornB.sf)) %>%
  filter(Year == 2010)
```

Because 2010 ACS data is not working with tidycensus. Try 2015 tornadoes instead.
```{r}
Torn2015.sf <- TornB.sf %>%
  mutate(ID = 1:nrow(TornB.sf)) %>%
  filter(Year == 2015)

Torn2013.sf <- TornB.sf %>%
  mutate(ID = 1:nrow(TornB.sf)) %>%
  filter(Year == 2013)
```

Get Census tracts for all contiguous U.S. states. Begin with 2017 tract-level data.
```{r}
us <- unique(fips_codes$state)[c(1, 3:8, 10:11, 13:51)]

counties <- fips_codes %>%
  filter(state %in% us)

options(tigris_use_cache = TRUE)

us2017.sf = reduce(map(us, function(x) {
  get_acs(state = x,
          geography = "tract", 
          variables = ("B01001_001E"),
          year = 2017,
          output = "wide",
          geometry = TRUE)
}),
rbind
)

us2017.sf <- st_transform(us2017.sf, crs = st_crs(Torn2010.sf))
```

Then collect 2010 ACS tract-level data. Current bug with 2010--2014 ACS 5-years estimates.
```{r}
us2010.sf <- reduce(map(us, function(x) {
  get_acs(state = x,
          county = county_state$COUNTYFP,
          geography = "tract", 
          variables = ("B01001_001E"),
          year = 2011,
          output = "wide",
          geometry = TRUE)
}),
rbind
)

us2010.sf <- st_transform(us2010.sf, crs = st_crs(Torn2010.sf))
```

Try a work-around. This takes a long time. Split the census tracts into multiple sfdf (otherwise Census API will time out).

Create a function to get data by state.
```{r}
get_state_demographic_data <- function(the_state, the_year) {

# get all counties in given state
  county_state <- tigris::counties(
  state = the_state,
  cb = TRUE,
  resolution = "20m",
  year = "2010",
  class = "sf",
  refresh=TRUE
)
  
# Execute the code
  us2010A.sf <- reduce(
    map2(
    .x = county_state$STATEFP,
    .y = county_state$COUNTYFP,
    ~ get_acs(
        state = .x,
        county = .y,
        geography = "tract", 
        variables = c("B01001_001", "B01001_002", "B01001_003", "B01001_004", "B01001_005", "B01001_006", "B01001_007", "B01001_008", "B01001_009", "B01001_010", "B01001_011", "B01001_012", "B01001_013", "B01001_014", "B01001_015", "B01001_016", "B01001_017", "B01001_018", "B01001_019", "B01001_020", "B01001_021", "B01001_022", "B01001_023", "B01001_024", "B01001_025", "B01001_026", "B01001_027", "B01001_028", "B01001_029", "B01001_030", "B01001_031", "B01001_032", "B01001_033", "B01001_034", "B01001_035", "B01001_036", "B01001_037", "B01001_038", "B01001_039", "B01001_040", "B01001_041", "B01001_042", "B01001_043", "B01001_044", "B01001_045", "B01001_046", "B01001_047", "B01001_048", "B01001_049", "B02001_002", "B02001_003", "B19013_001", "B25024_001", "B25024_010"),
        year = 2010,
        output = "wide",
        geometry = TRUE
        )
      ),
    rbind
   )
}
  
AL.df = get_state_demographic_data("AL", 2010)  
```

Try using tidycensus. Start with only 9 states.
```{r}
my_states <- unique(fips_codes$state)[c(1, 3:8, 10:11)]

county_state <- tigris::counties(
  state = my_states,
  cb = TRUE,
  resolution = "20m",
  year = "2010",
  class = "sf",
  refresh=TRUE
)

us2010A.sf <- reduce(
    map2(
    .x = county_state$STATEFP,
    .y = county_state$COUNTYFP,
    ~ get_acs(
        state = .x,
        county = .y,
        geography = "tract", 
        variables = c("B01001_001", "B01001_002", "B01001_003", "B01001_004", "B01001_005", "B01001_006", "B01001_007", "B01001_008", "B01001_009", "B01001_010", "B01001_011", "B01001_012", "B01001_013", "B01001_014", "B01001_015", "B01001_016", "B01001_017", "B01001_018", "B01001_019", "B01001_020", "B01001_021", "B01001_022", "B01001_023", "B01001_024", "B01001_025", "B01001_026", "B01001_027", "B01001_028", "B01001_029", "B01001_030", "B01001_031", "B01001_032", "B01001_033", "B01001_034", "B01001_035", "B01001_036", "B01001_037", "B01001_038", "B01001_039", "B01001_040", "B01001_041", "B01001_042", "B01001_043", "B01001_044", "B01001_045", "B01001_046", "B01001_047", "B01001_048", "B01001_049", "B02001_002", "B02001_003", "B19013_001", "B25024_001", "B25024_010"),
        year = 2010,
        output = "wide",
        geometry = TRUE
        )
      ),
    rbind
   )
```

Then add in the next 20 states.
```{r}
my_states <- unique(fips_codes$state)[c(13:33)]

county_state <- tigris::counties(
  state = my_states,
  cb = TRUE,
  resolution = "20m",
  year = "2010",
  class = "sf"
)

us2010B.sf <- reduce(
    map2(
    .x = county_state$STATEFP,
    .y = county_state$COUNTYFP,
    ~ get_acs(
        state = .x,
        county = .y,
        geography = "tract", 
        variables = ("B01001_001E"),
        year = 2010,
        output = "wide",
        geometry = TRUE
        )
      ),
    rbind
   )
```

Then add the last 20 states
```{r}
my_states <- unique(fips_codes$state)[c(34:51)]

county_state <- tigris::counties(
  state = my_states,
  cb = TRUE,
  resolution = "20m",
  year = "2010",
  class = "sf"
)

us2010C.sf <- reduce(
    map2(
    .x = county_state$STATEFP,
    .y = county_state$COUNTYFP,
    ~ get_acs(
        state = .x,
        county = .y,
        geography = "tract", 
        variables = ("B01001_001E"),
        year = 2010,
        output = "wide",
        geometry = TRUE
        )
      ),
    rbind
   )
```

Combine the simple features dataframes.
```{r}
x <-  rbind(us2010.sf, us2010B.sf)
us2010.sf <-  rbind(x, us2010C.sf)

us2010.sf <- st_transform(us2010.sf, crs = st_crs(Torn2010.sf))
```


Then collect 2000 Census tract-level data.
```{r}
us2000.sf = reduce(map(us, function(x) {
  get_decennial(state = x,
          geography = "tract", 
          variables = ("P001001"),
          year = 2000,
          output = "wide",
          geometry = TRUE)
}),
rbind
)

us2000.sf <- st_transform(us2000.sf, crs = st_crs(Torn2010.sf))
```

Then collect 1990 Census tract-level data. There is a bug in the tidycensus package related to changes in tract and county differences. For now, use county level data.
```{r}
us1990.sf = reduce(map(us, function(x) {
  get_decennial(state = x,
          geography = "tract",
          variables = ("P0010001"),
          year = 1990,
          output = "wide",
          geometry = TRUE)
}),
rbind
)

us1990.sf <- st_transform(us1990.sf, crs = st_crs(Torn2010.sf))
```

Find percent error for different years. Begin by interpolating estimates across the 2015 ACS, 2000 Census, and 1990 Census.
```{r}
# 2015
out2015 <- aw_interpolate(Torn2016.sf, 
               tid = "ID", 
               source = us2010.sf, 
               sid = "GEOID", 
               extensive = "B01001_001E",
               weight = "total", 
               output = "sf")

# 2000
out2000 <- aw_interpolate(Torn2016.sf, 
               tid = "ID", 
               source = us2000.sf, 
               sid = "GEOID", 
               extensive = "P001001",
               weight = "total", 
               output = "sf")

# 1990
out1990 <- aw_interpolate(Torn2016.sf, 
               tid = "ID", 
               source = us1990.sf, 
               sid = "GEOID", 
               extensive = "P0010001",
               weight = "total", 
               output = "sf")
```

```{r}
# 2014 tornado paths
df <-  data.frame(x = c(2015, 2000, 1990), 
                y = c((abs(sum(out2015$B01001_001E) - sum(out2015$B01001_001E))/ sum(out2015$B01001_001E)),  (abs(sum(out2000$P001001) - sum(out2015$B01001_001E))/ sum(out2000$P001001)), (abs(sum(out1990$P0010001) - sum(out2015$B01001_001E))/ sum(out1990$P0010001))))

df$y <- df$y * 100

# 2013 tornado paths (rerun code)
df2 <-  data.frame(x = c(2015, 2000, 1990), 
                y = c((abs(sum(out2015$B01001_001E[-57]) - sum(out2015$B01001_001E[-57]))/ sum(out2015$B01001_001E[-57])),  (abs(sum(out2000$P001001[-57]) - sum(out2015$B01001_001E[-57]))/ sum(out2000$P001001[-57])), (abs(sum(out1990$P0010001[-57]) - sum(out2015$B01001_001E[-57]))/ sum(out1990$P0010001[-57]))))

df2$y <- df2$y * 100

# 2012 tornado paths (rerun code)

df3 <-  data.frame(x = c(2015, 2000, 1990), 
                y = c((abs(sum(out2015$B01001_001E) - sum(out2015$B01001_001E))/ sum(out2015$B01001_001E)),  (abs(sum(out2000$P001001) - sum(out2015$B01001_001E))/ sum(out2000$P001001)), (abs(sum(out1990$P0010001) - sum(out2015$B01001_001E))/ sum(out1990$P0010001))))

df3$y <- df3$y * 100

# 2016 tornado paths (rerun code)

df4 <-  data.frame(x = c(2015, 2000, 1990), 
                y = c((abs(sum(out2015$B01001_001E) - sum(out2015$B01001_001E))/ sum(out2015$B01001_001E)),  (abs(sum(out2000$P001001) - sum(out2015$B01001_001E))/ sum(out2000$P001001)), (abs(sum(out1990$P0010001) - sum(out2015$B01001_001E))/ sum(out1990$P0010001))))

df4$y <- df4$y * 100

# Plot
ggplot() +
  geom_point(data = df2, aes(x = x, y = y, colour = "red")) +
  geom_point(data = df3, aes(x = x, y = y, colour = "blue")) +
  geom_point(data = df4, aes(x = x, y = y, colour = "green")) + 
  geom_point(data = df, aes(x = x, y = y)) +
  ylab("Percent Error") + xlab("Year of Data") + 
  theme_minimal() +
  theme(legend.position="none")

                
sum(out2015$B01001_001E)
sum(out2000$P001001)
sum(out1990$P0010001)
```







### OLD CODE

Add tornado paths by buffering tornado tracks by associated widths. Need to account for tornadoes without a path (same latitude and longitude).
```{r}
begin.coord <- data.frame(lon=Torn.sf$slon, lat=Torn.sf$slat)
end.coord <- data.frame(lon=Torn.sf$elon, lat=Torn.sf$elat)

l_sf <- vector("list", nrow(Torn.sf))
for (i in seq_along(l_sf)){
  l_sf[[i]] <- st_linestring(as.matrix(rbind(begin.coord[i, ], end.coord[i,])))
}
# Create simple feature geometry list column
l_sfc <- st_sfc(l_sf, crs = "+proj=longlat +datum=WGS84")
# Put new bounding boxes into the geometry column in the sf database and transform the simple features to new projection
Torn.sf$geometry <- l_sfc
st_transform(Torn.sf, crs = "+proj=lcc +lat_1=33 +lat_2=45 +lat_0=39 +lon_0=-96 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0")
```

```{r}
TornSLine = as(Torn.sf, "Spatial")
TornSLine = spTransform(TornSLine, CRS("+proj=lcc +lat_1=33 +lat_2=45 +lat_0=39 +lon_0=-96 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0"))
TornSPoly = gBuffer(TornSLine, byid = TRUE, width = TornSLine$Width/2, capStyle = "SQUARE")
TornSPoly.sf <- st_as_sf(TornSPoly)
```

Calculate the error in area between the estimated paths (AreaPath variable) and the buffered tornado tracks (tornado paths).
```{r}
((sum(Torn.sf$AreaPath) - gArea(TornSPoly))/sum(Torn.sf$AreaPath)) * 100
```

The error is less than 1/10 a percent. The buffer method works.

Plot casualty-producing tornadoes.
```{r}
sts <- state.name[!state.name %in% c("Alaska", "Hawaii")]
stateBorders <- us_states(states = sts)

tm_shape(stateBorders, projection ="+init=epsg:2163") +
  tm_borders() +
  tm_fill(col = "grey94") +
  tm_shape(TornSPoly) +
  tm_polygons(col = "black") +
  tm_format('World', legend.position = c("left", "bottom"),
                   attr.position = c("left", "bottom"),
                  legend.frame = FALSE) +
  #tm_format_Europe(legend.position = c("left", "bottom"),
  #                 attr.position = c("left", "bottom"),
  #                 legend.frame = TRUE) +
  tm_scale_bar(position = c("right", "bottom")) +
  #tm_compass(position = c("right", "bottom")) +
  tm_layout(frame = FALSE, attr.outside=TRUE)
```

Plot casualty-producing tornadoes in Ohio over census tracts.
```{r}
sts <- state.name[!state.name %in% c("Alaska", "Hawaii")]
stateBorders <- us_states(states = sts)
OH = stateBorders[stateBorders$stusps == "OH",]

tm_shape(stateBorders[stateBorders$statefp == 39,], projection ="+init=epsg:2163") +
  tm_borders() +
  tm_fill(col = "grey94") +
  tm_shape(TornOH.sf[10,]) +
  tm_polygons(col = "blue") +
  tm_shape(OH2010.sfdf) + 
  tm_polygons(col = "grey98", alpha = 0.2) +
  tm_format('World', legend.position = c("left", "bottom"),
                   attr.position = c("left", "bottom"),
                  legend.frame = FALSE) +
  #tm_format_Europe(legend.position = c("left", "bottom"),
  #                 attr.position = c("left", "bottom"),
  #                 legend.frame = TRUE) +
  tm_scale_bar(position = c("right", "bottom")) +
  #tm_compass(position = c("right", "bottom")) +
  tm_layout(frame = FALSE, attr.outside=TRUE)
```

```{r}
get_state_demographic_data <- function(the_state, the_year) {
  
  options(tigris_use_cache = TRUE) #keep chached version of data
  
  # get all counties in given state
  counties <- tidycensus::fips_codes %>%
    dplyr::filter(state == the_state)
  
  # loop over counties and get tracts for each county
  purrr::map(counties$county_code, 
       ~ get_acs(
         geography = 'tract',
         table = c("B19301"),
         state = the_state,
         county = .x,
         year = the_year,
         survey = 'acs5',
         geometry = TRUE)
       ) %>% 
    purrr::reduce(rbind)  # bind rows of all counties
}

get_state_demographic_data(us, 2010)
```
